{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# Performance Benchmarks\n\nTopics: Speed comparison, accuracy testing, backend selection, model efficiency\nTime: 15 minutes\nPrerequisites: 02_models_comparison.py, 01_basic_operations.py\nRelated: 32_distributed_representations.py, 02_models_comparison.py\n\nThis example benchmarks different VSA models and backends to help you choose\nthe right configuration for your application's performance requirements.\n\nKey concepts:\n- Operation speed: bind, bundle, permute, similarity\n- Backend comparison: NumPy (CPU) vs PyTorch (GPU) vs JAX (JIT)\n- Model efficiency: Memory and computation trade-offs\n- Dimension scaling: How performance changes with dimension\n- Practical recommendations: Choose based on your constraints\n\nUse this to make informed decisions about model and backend selection.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import time\nimport numpy as np\nfrom holovec import VSA\n\nprint(\"=\" * 70)\nprint(\"Performance Benchmarks\")\nprint(\"=\" * 70)\nprint()\n\n# ============================================================================\n# Demo 1: Operation Speed by Model\n# ============================================================================\nprint(\"=\" * 70)\nprint(\"Demo 1: Basic Operation Speed (NumPy backend)\")\nprint(\"=\" * 70)\n\ndimension = 10000\nn_iterations = 1000\n\nmodels_to_test = ['MAP', 'FHRR', 'HRR', 'BSC']\n\nprint(f\"\\nDimension: {dimension}\")\nprint(f\"Iterations: {n_iterations}\")\nprint(f\"Backend: NumPy (CPU)\")\nprint()\n\nresults = {}\n\nfor model_name in models_to_test:\n    model = VSA.create(model_name, dim=dimension, seed=42)\n\n    # Create test vectors\n    A = model.random(seed=1)\n    B = model.random(seed=2)\n    vectors = [model.random(seed=i) for i in range(10)]\n\n    # Benchmark bind\n    start = time.time()\n    for _ in range(n_iterations):\n        _ = model.bind(A, B)\n    bind_time = (time.time() - start) / n_iterations * 1000  # ms\n\n    # Benchmark bundle\n    start = time.time()\n    for _ in range(n_iterations):\n        _ = model.bundle(vectors)\n    bundle_time = (time.time() - start) / n_iterations * 1000\n\n    # Benchmark similarity\n    start = time.time()\n    for _ in range(n_iterations):\n        _ = model.similarity(A, B)\n    sim_time = (time.time() - start) / n_iterations * 1000\n\n    # Benchmark permute (if available)\n    try:\n        start = time.time()\n        for _ in range(n_iterations):\n            _ = model.permute(A)\n        perm_time = (time.time() - start) / n_iterations * 1000\n    except:\n        perm_time = None\n\n    results[model_name] = {\n        'bind': bind_time,\n        'bundle': bundle_time,\n        'similarity': sim_time,\n        'permute': perm_time\n    }\n\n# Print results\nprint(f\"{'Model':<10s} {'Bind (ms)':<12s} {'Bundle (ms)':<12s} {'Sim (ms)':<12s} {'Permute (ms)':<12s}\")\nprint(\"-\" * 70)\n\nfor model_name, times in results.items():\n    perm_str = f\"{times['permute']:.4f}\" if times['permute'] else \"N/A\"\n    print(f\"{model_name:<10s} {times['bind']:10.4f}   {times['bundle']:10.4f}   \"\n          f\"{times['similarity']:10.4f}   {perm_str:>10s}\")\n\nprint(\"\\nObservations:\")\nprint(\"  - MAP typically fastest (simple multiplication)\")\nprint(\"  - FHRR/HRR slower (FFT operations)\")\nprint(\"  - BSC depends on sparsity (fewer operations on sparse vectors)\")\n\n# ============================================================================\n# Demo 2: Dimension Scaling\n# ============================================================================\nprint(\"\\n\" + \"=\" * 70)\nprint(\"Demo 2: Performance vs Dimension\")\nprint(\"=\" * 70)\n\ndimensions = [1000, 5000, 10000, 20000]\nmodel_name = 'MAP'  # Test with MAP (fastest)\n\nprint(f\"\\nModel: {model_name}\")\nprint(f\"\\n{'Dimension':<12s} {'Bind (ms)':<12s} {'Bundle (ms)':<12s} {'Similarity (ms)':<15s}\")\nprint(\"-\" * 60)\n\nfor dim in dimensions:\n    model = VSA.create(model_name, dim=dim, seed=42)\n    A = model.random(seed=1)\n    B = model.random(seed=2)\n    vectors = [model.random(seed=i) for i in range(10)]\n\n    # Quick benchmark (fewer iterations for larger dims)\n    n_iter = max(100, 10000 // (dim // 1000))\n\n    # Bind\n    start = time.time()\n    for _ in range(n_iter):\n        _ = model.bind(A, B)\n    bind_time = (time.time() - start) / n_iter * 1000\n\n    # Bundle\n    start = time.time()\n    for _ in range(n_iter):\n        _ = model.bundle(vectors)\n    bundle_time = (time.time() - start) / n_iter * 1000\n\n    # Similarity\n    start = time.time()\n    for _ in range(n_iter):\n        _ = model.similarity(A, B)\n    sim_time = (time.time() - start) / n_iter * 1000\n\n    print(f\"{dim:<12d} {bind_time:10.4f}   {bundle_time:10.4f}   {sim_time:12.4f}\")\n\nprint(\"\\nScaling pattern:\")\nprint(\"  - Generally linear with dimension\")\nprint(\"  - Bundle scales with number of vectors to combine\")\nprint(\"  - Similarity involves dot product (linear complexity)\")\n\n# ============================================================================\n# Demo 3: Memory Usage\n# ============================================================================\nprint(\"\\n\" + \"=\" * 70)\nprint(\"Demo 3: Memory Footprint\")\nprint(\"=\" * 70)\n\ndimension = 10000\n\nprint(f\"\\nDimension: {dimension}\")\nprint(f\"\\n{'Model':<10s} {'Dtype':<15s} {'Bytes/Vector':<15s} {'MB/1000 vectors':<15s}\")\nprint(\"-\" * 65)\n\nfor model_name in ['MAP', 'FHRR', 'HRR', 'BSC']:\n    model = VSA.create(model_name, dim=dimension, seed=42)\n    A = model.random(seed=1)\n\n    # Get dtype info\n    if hasattr(A, 'dtype'):\n        dtype = str(A.dtype)\n        itemsize = A.itemsize if hasattr(A, 'itemsize') else 8\n    else:\n        dtype = \"backend-specific\"\n        itemsize = 8  # estimate\n\n    bytes_per_vector = dimension * itemsize\n    mb_per_1000 = bytes_per_vector * 1000 / (1024 * 1024)\n\n    print(f\"{model_name:<10s} {dtype:<15s} {bytes_per_vector:<15,d} {mb_per_1000:14.2f}\")\n\nprint(\"\\nMemory considerations:\")\nprint(\"  - MAP: int8 or float32 (smallest)\")\nprint(\"  - FHRR/HRR: complex64/128 (larger, 2x float)\")\nprint(\"  - BSC: Binary sparse (very small if sparse)\")\nprint(\"  - Choose based on storage constraints\")\n\n# ============================================================================\n# Demo 4: Accuracy Under Noise\n# ============================================================================\nprint(\"\\n\" + \"=\" * 70)\nprint(\"Demo 4: Noise Tolerance Comparison\")\nprint(\"=\" * 70)\n\ndimension = 10000\nnoise_levels = [0.0, 0.1, 0.2, 0.3, 0.4, 0.5]\n\nprint(f\"\\nDimension: {dimension}\")\nprint(f\"Test: similarity(A, A + noise)\")\nprint()\n\nprint(f\"{'Noise':<10s} \", end=\"\")\nfor model_name in models_to_test:\n    print(f\"{model_name:<10s} \", end=\"\")\nprint()\nprint(\"-\" * 55)\n\nfor noise_level in noise_levels:\n    print(f\"{noise_level:<10.1f} \", end=\"\")\n\n    for model_name in models_to_test:\n        model = VSA.create(model_name, dim=dimension, seed=42)\n        A = model.random(seed=1)\n\n        # Add noise\n        noise = model.random(seed=999)\n        noisy_A = model.bundle([A, noise])  # Simple noise addition via bundling\n\n        # Measure similarity to original\n        sim = float(model.similarity(A, noisy_A))\n        print(f\"{sim:10.3f} \", end=\"\")\n\n    print()\n\nprint(\"\\nNoise tolerance:\")\nprint(\"  - All models degrade gracefully with noise\")\nprint(\"  - Higher dimension = better noise tolerance\")\nprint(\"  - Use cleanup strategies for noise-heavy applications\")\n\n# ============================================================================\n# Demo 5: Bundling Capacity\n# ============================================================================\nprint(\"\\n\" + \"=\" * 70)\nprint(\"Demo 5: Bundling Capacity (Information Loss)\")\nprint(\"=\" * 70)\n\ndimension = 10000\nbundle_sizes = [1, 5, 10, 20, 50, 100]\n\nprint(f\"\\nDimension: {dimension}\")\nprint(f\"Test: similarity after bundling N vectors\")\nprint()\n\nprint(f\"{'N vectors':<12s} \", end=\"\")\nfor model_name in models_to_test:\n    print(f\"{model_name:<10s} \", end=\"\")\nprint()\nprint(\"-\" * 60)\n\nfor n in bundle_sizes:\n    print(f\"{n:<12d} \", end=\"\")\n\n    for model_name in models_to_test:\n        model = VSA.create(model_name, dim=dimension, seed=42)\n\n        # Create target and other vectors\n        target = model.random(seed=1)\n        others = [model.random(seed=100+i) for i in range(1, n)]\n\n        # Bundle\n        if n == 1:\n            bundled = target\n        else:\n            bundled = model.bundle([target] + others)\n\n        # Similarity to original\n        sim = float(model.similarity(bundled, target))\n        print(f\"{sim:10.3f} \", end=\"\")\n\n    print()\n\nprint(\"\\nCapacity insights:\")\nprint(\"  - Similarity degrades as bundle size increases\")\nprint(\"  - MAP maintains higher similarity (sum-based)\")\nprint(\"  - Higher dimension supports more vectors in bundle\")\n\n# ============================================================================\n# Demo 6: Backend Comparison (if available)\n# ============================================================================\nprint(\"\\n\" + \"=\" * 70)\nprint(\"Demo 6: Backend Comparison\")\nprint(\"=\" * 70)\n\navailable_backends = []\n\n# Test numpy\ntry:\n    model = VSA.create('MAP', dim=10000, backend='numpy', seed=42)\n    available_backends.append('numpy')\nexcept:\n    pass\n\n# Test torch (if available)\ntry:\n    model = VSA.create('MAP', dim=10000, backend='torch', seed=42)\n    available_backends.append('torch')\nexcept:\n    pass\n\n# Test jax (if available)\ntry:\n    model = VSA.create('MAP', dim=10000, backend='jax', seed=42)\n    available_backends.append('jax')\nexcept:\n    pass\n\nprint(f\"\\nAvailable backends: {', '.join(available_backends)}\")\n\nif len(available_backends) > 1:\n    print(\"\\nBenchmarking available backends...\")\n    dimension = 10000\n    n_iter = 100\n\n    print(f\"\\n{'Backend':<10s} {'Bind (ms)':<12s} {'Bundle (ms)':<12s} {'Similarity (ms)':<15s}\")\n    print(\"-\" * 60)\n\n    for backend in available_backends:\n        model = VSA.create('MAP', dim=dimension, backend=backend, seed=42)\n        A = model.random(seed=1)\n        B = model.random(seed=2)\n        vectors = [model.random(seed=i) for i in range(10)]\n\n        # Bind\n        start = time.time()\n        for _ in range(n_iter):\n            _ = model.bind(A, B)\n        bind_time = (time.time() - start) / n_iter * 1000\n\n        # Bundle\n        start = time.time()\n        for _ in range(n_iter):\n            _ = model.bundle(vectors)\n        bundle_time = (time.time() - start) / n_iter * 1000\n\n        # Similarity\n        start = time.time()\n        for _ in range(n_iter):\n            _ = model.similarity(A, B)\n        sim_time = (time.time() - start) / n_iter * 1000\n\n        print(f\"{backend:<10s} {bind_time:10.4f}   {bundle_time:10.4f}   {sim_time:12.4f}\")\nelse:\n    print(f\"\\nOnly {available_backends[0]} backend available.\")\n    print(\"\\nTo test other backends:\")\n    print(\"  pip install torch  # For GPU acceleration\")\n    print(\"  pip install jax jaxlib  # For JIT compilation\")\n\nprint(\"\\nBackend recommendations:\")\nprint(\"  - NumPy: Default, good for CPU, no extra dependencies\")\nprint(\"  - PyTorch: Best for GPU, large batches, deep learning integration\")\nprint(\"  - JAX: Best for JIT compilation, TPU, functional programming\")\n\n# ============================================================================\n# Summary\n# ============================================================================\nprint(\"\\n\" + \"=\" * 70)\nprint(\"Summary: Performance Recommendations\")\nprint(\"=\" * 70)\nprint()\n\nprint(\"\u2713 Model Selection by Speed:\")\nprint(\"  1. MAP - Fastest (element-wise multiplication)\")\nprint(\"  2. BSC - Fast for sparse operations\")\nprint(\"  3. HRR/FHRR - Slower (FFT overhead)\")\nprint()\n\nprint(\"\u2713 Dimension Recommendations:\")\nprint(\"  - Small problems (<1000 items): 1000-5000 dim\")\nprint(\"  - Medium problems (1000-10000 items): 5000-10000 dim\")\nprint(\"  - Large problems (>10000 items): 10000-20000 dim\")\nprint()\n\nprint(\"\u2713 Backend Selection:\")\nprint(\"  - CPU only: NumPy (default)\")\nprint(\"  - GPU available: PyTorch (faster for large batches)\")\nprint(\"  - Need JIT/TPU: JAX (compile once, run fast)\")\nprint()\n\nprint(\"\u2713 Memory Constraints:\")\nprint(\"  - Limited memory: MAP with lower dimension\")\nprint(\"  - Plenty of memory: Any model, higher dimension\")\nprint(\"  - Sparse data: BSC (efficient sparse storage)\")\nprint()\n\nprint(\"\u2713 Noise Tolerance:\")\nprint(\"  - High noise: Higher dimension, use cleanup strategies\")\nprint(\"  - Low noise: Standard dimension (10000) sufficient\")\nprint()\n\nprint(\"Next steps:\")\nprint(\"  \u2192 32_distributed_representations.py - Capacity deep dive\")\nprint(\"  \u2192 33_error_handling_robustness.py - Noise handling strategies\")\nprint(\"  \u2192 02_models_comparison.py - Model characteristics\")\nprint()\nprint(\"=\" * 70)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}