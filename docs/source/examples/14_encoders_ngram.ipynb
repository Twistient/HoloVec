{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# Demonstration of N-gram Encoder for local sequence pattern encoding.\n\nThis demo showcases the NGramEncoder, which captures local patterns in\nsequences using sliding windows (n-grams). This is particularly useful for:\n\n- Text analysis (character/word n-grams)\n- Pattern matching in sequences\n- Similarity detection based on local context\n- NLP applications\n\nThe encoder supports:\n- Multiple n-gram sizes (unigrams, bigrams, trigrams, etc.)\n- Overlapping and non-overlapping windows (stride parameter)\n- Two modes: bundling (bag-of-ngrams) and chaining (ordered n-grams)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from holovec import VSA\nfrom holovec.encoders import NGramEncoder\n\n\ndef print_section(title):\n    \"\"\"Print a section header.\"\"\"\n    print(f\"\\n{'=' * 70}\")\n    print(f\"{title}\")\n    print('=' * 70)\n\n\ndef demo_basic_ngram_encoding():\n    \"\"\"Demonstrate basic n-gram encoding.\"\"\"\n    print_section(\"Demo 1: Basic N-gram Encoding\")\n\n    model = VSA.create('MAP', dim=5000, seed=42)\n    encoder = NGramEncoder(model, n=2, stride=1, mode='bundling', seed=42)\n\n    print(f\"\\nEncoder: {encoder}\")\n    print(f\"Configuration: n={encoder.n}, stride={encoder.stride}, mode='{encoder.mode}'\")\n\n    # Encode a sequence\n    sequence = ['the', 'quick', 'brown', 'fox']\n    hv = encoder.encode(sequence)\n\n    print(f\"\\nInput sequence: {sequence}\")\n    print(f\"Bigrams (n=2, stride=1):\")\n    print(\"  - ['the', 'quick']\")\n    print(\"  - ['quick', 'brown']\")\n    print(\"  - ['brown', 'fox']\")\n    print(f\"\\nEncoded hypervector shape: {hv.shape}\")\n    print(f\"Codebook size: {encoder.get_codebook_size()} unique symbols\")\n\n\ndef demo_different_n_values():\n    \"\"\"Demonstrate different n-gram sizes.\"\"\"\n    print_section(\"Demo 2: Different N-gram Sizes\")\n\n    model = VSA.create('MAP', dim=5000, seed=42)\n    sequence = ['A', 'B', 'C', 'D', 'E']\n\n    print(f\"\\nInput sequence: {sequence}\\n\")\n\n    for n in [1, 2, 3, 4]:\n        encoder = NGramEncoder(model, n=n, stride=1, mode='bundling', seed=42)\n        hv = encoder.encode(sequence)\n\n        # Calculate number of n-grams\n        num_ngrams = len(sequence) - n + 1\n\n        name = {1: \"Unigrams\", 2: \"Bigrams\", 3: \"Trigrams\", 4: \"4-grams\"}[n]\n        print(f\"{name} (n={n}):\")\n        print(f\"  Number of n-grams: {num_ngrams}\")\n        print(f\"  Hypervector shape: {hv.shape}\")\n\n\ndef demo_stride_parameter():\n    \"\"\"Demonstrate stride (overlapping vs non-overlapping).\"\"\"\n    print_section(\"Demo 3: Stride Parameter (Overlapping vs Non-overlapping)\")\n\n    model = VSA.create('MAP', dim=5000, seed=42)\n    sequence = ['A', 'B', 'C', 'D', 'E', 'F']\n\n    print(f\"\\nInput sequence: {sequence}\\n\")\n\n    # Overlapping bigrams (stride=1)\n    encoder1 = NGramEncoder(model, n=2, stride=1, mode='bundling', seed=42)\n    hv1 = encoder1.encode(sequence)\n\n    print(\"Overlapping bigrams (stride=1):\")\n    print(\"  N-grams: ['A','B'], ['B','C'], ['C','D'], ['D','E'], ['E','F']\")\n    print(f\"  Count: 5 n-grams\")\n\n    # Non-overlapping bigrams (stride=2)\n    encoder2 = NGramEncoder(model, n=2, stride=2, mode='bundling', seed=42)\n    hv2 = encoder2.encode(sequence)\n\n    print(\"\\nNon-overlapping bigrams (stride=2):\")\n    print(\"  N-grams: ['A','B'], ['C','D'], ['E','F']\")\n    print(f\"  Count: 3 n-grams\")\n\n    # Partial overlap (stride=2 with trigrams)\n    encoder3 = NGramEncoder(model, n=3, stride=2, mode='bundling', seed=42)\n    hv3 = encoder3.encode(sequence)\n\n    print(\"\\nPartial overlap trigrams (n=3, stride=2):\")\n    print(\"  N-grams: ['A','B','C'], ['C','D','E']\")\n    print(f\"  Count: 2 n-grams\")\n\n\ndef demo_text_similarity():\n    \"\"\"Demonstrate text similarity using n-grams.\"\"\"\n    print_section(\"Demo 4: Text Similarity with N-grams\")\n\n    model = VSA.create('MAP', dim=10000, seed=42)\n    encoder = NGramEncoder(model, n=2, stride=1, mode='bundling', seed=42)\n\n    # Encode sentences as word bigrams\n    sent1 = ['the', 'cat', 'sat', 'on', 'the', 'mat']\n    sent2 = ['the', 'cat', 'sat', 'on', 'the', 'hat']  # Similar (1 word diff)\n    sent3 = ['a', 'dog', 'ran', 'in', 'the', 'park']   # Different\n\n    hv1 = encoder.encode(sent1)\n    hv2 = encoder.encode(sent2)\n    hv3 = encoder.encode(sent3)\n\n    print(\"\\nSentence 1:\", ' '.join(sent1))\n    print(\"Sentence 2:\", ' '.join(sent2), \"(differs by 1 word)\")\n    print(\"Sentence 3:\", ' '.join(sent3), \"(completely different)\\n\")\n\n    sim_1_2 = float(model.similarity(hv1, hv2))\n    sim_1_3 = float(model.similarity(hv1, hv3))\n\n    print(f\"Similarity (sent1 vs sent2): {sim_1_2:.3f}\")\n    print(f\"Similarity (sent1 vs sent3): {sim_1_3:.3f}\")\n\n    print(\"\\nKey insight:\")\n    print(\"  Sentences sharing more bigrams have higher similarity.\")\n    print(\"  Bigrams shared by sent1 and sent2:\")\n    print(\"    ['the','cat'], ['cat','sat'], ['sat','on'], ['on','the']\")\n\n\ndef demo_bundling_vs_chaining():\n    \"\"\"Demonstrate bundling mode vs chaining mode.\"\"\"\n    print_section(\"Demo 5: Bundling vs Chaining Modes\")\n\n    model = VSA.create('MAP', dim=10000, seed=42)\n\n    sequence = ['A', 'B', 'C']\n\n    # Bundling mode: order-invariant across n-grams\n    encoder_bundle = NGramEncoder(model, n=2, stride=1, mode='bundling', seed=42)\n    hv_bundle = encoder_bundle.encode(sequence)\n\n    print(\"Bundling Mode (bag-of-ngrams):\")\n    print(f\"  Sequence: {sequence}\")\n    print(f\"  N-grams: ['A','B'], ['B','C']\")\n    print(f\"  Encoding: bundle(encode(['A','B']), encode(['B','C']))\")\n    print(f\"  Order-sensitive: No (n-grams bundled)\")\n    print(f\"  Reversible: {encoder_bundle.is_reversible}\")\n\n    # Chaining mode: order-sensitive\n    encoder_chain = NGramEncoder(model, n=2, stride=1, mode='chaining', seed=42)\n    hv_chain = encoder_chain.encode(sequence)\n\n    print(\"\\nChaining Mode (ordered n-grams):\")\n    print(f\"  Sequence: {sequence}\")\n    print(f\"  N-grams: ['A','B'] at position 0, ['B','C'] at position 1\")\n    print(f\"  Encoding: bundle(permute(encode(['A','B']), 0), permute(encode(['B','C']), 1))\")\n    print(f\"  Order-sensitive: Yes (positions encoded)\")\n    print(f\"  Reversible: {encoder_chain.is_reversible}\")\n\n    # Test decoding in chaining mode\n    if encoder_chain.is_reversible:\n        decoded = encoder_chain.decode(hv_chain, max_ngrams=3, threshold=0.2)\n        print(f\"\\nDecoded n-grams (approximate): {decoded}\")\n\n\ndef demo_character_ngrams():\n    \"\"\"Demonstrate character-level n-grams.\"\"\"\n    print_section(\"Demo 6: Character-Level N-grams\")\n\n    model = VSA.create('MAP', dim=10000, seed=42)\n    encoder = NGramEncoder(model, n=3, stride=1, mode='bundling', seed=42)\n\n    # Encode words as character trigrams\n    word1 = list(\"pattern\")   # ['p','a','t','t','e','r','n']\n    word2 = list(\"patter\")    # ['p','a','t','t','e','r']\n    word3 = list(\"matter\")    # ['m','a','t','t','e','r']\n\n    hv1 = encoder.encode(word1)\n    hv2 = encoder.encode(word2)\n    hv3 = encoder.encode(word3)\n\n    print(\"\\nWord 1: 'pattern'\")\n    print(\"  Trigrams: pat, att, tte, ter, ern\")\n\n    print(\"\\nWord 2: 'patter'\")\n    print(\"  Trigrams: pat, att, tte, ter\")\n    print(\"  (shares 4/5 with 'pattern')\")\n\n    print(\"\\nWord 3: 'matter'\")\n    print(\"  Trigrams: mat, att, tte, ter\")\n    print(\"  (shares 3/5 with 'pattern')\")\n\n    sim_1_2 = float(model.similarity(hv1, hv2))\n    sim_1_3 = float(model.similarity(hv1, hv3))\n\n    print(f\"\\nSimilarity 'pattern' vs 'patter': {sim_1_2:.3f}\")\n    print(f\"Similarity 'pattern' vs 'matter': {sim_1_3:.3f}\")\n\n    print(\"\\nKey insight:\")\n    print(\"  Character n-grams capture sub-word similarity\")\n    print(\"  Useful for misspelling detection and fuzzy matching\")\n\n\ndef demo_application_text_classification():\n    \"\"\"Demonstrate application: text classification.\"\"\"\n    print_section(\"Demo 7: Application - Text Classification\")\n\n    model = VSA.create('MAP', dim=10000, seed=42)\n    encoder = NGramEncoder(model, n=2, stride=1, mode='bundling', seed=42)\n\n    print(\"\\nScenario: Classify sentences as positive or negative\\n\")\n\n    # Training examples\n    positive_examples = [\n        ['i', 'love', 'this', 'product'],\n        ['great', 'quality', 'and', 'service'],\n        ['highly', 'recommend', 'this', 'item']\n    ]\n\n    negative_examples = [\n        ['terrible', 'quality', 'and', 'service'],\n        ['do', 'not', 'recommend', 'this'],\n        ['very', 'poor', 'experience']\n    ]\n\n    # Create class prototypes by bundling examples\n    positive_hvs = [encoder.encode(ex) for ex in positive_examples]\n    negative_hvs = [encoder.encode(ex) for ex in negative_examples]\n\n    positive_prototype = model.bundle(positive_hvs)\n    negative_prototype = model.bundle(negative_hvs)\n\n    print(\"Training Data:\")\n    print(\"  Positive examples: 3\")\n    print(\"  Negative examples: 3\")\n\n    # Test examples\n    test_sentences = [\n        (['i', 'recommend', 'this', 'product'], \"Positive\"),\n        (['poor', 'quality', 'item'], \"Negative\"),\n        (['great', 'experience'], \"Positive\"),\n    ]\n\n    print(\"\\nTest Results:\")\n    for sentence, true_label in test_sentences:\n        hv = encoder.encode(sentence)\n\n        sim_positive = float(model.similarity(hv, positive_prototype))\n        sim_negative = float(model.similarity(hv, negative_prototype))\n\n        predicted = \"Positive\" if sim_positive > sim_negative else \"Negative\"\n\n        print(f\"\\n  Sentence: {' '.join(sentence)}\")\n        print(f\"  Sim to positive: {sim_positive:.3f}\")\n        print(f\"  Sim to negative: {sim_negative:.3f}\")\n        print(f\"  Predicted: {predicted}, True: {true_label} \"\n              f\"{'\u2713' if predicted == true_label else '\u2717'}\")\n\n\ndef main():\n    \"\"\"Run all demos.\"\"\"\n    print(\"=\" * 70)\n    print(\"N-gram Encoder - Comprehensive Demonstration\")\n    print(\"=\" * 70)\n    print(\"\\nThe NGramEncoder captures local patterns in sequences using\")\n    print(\"sliding windows. This is essential for:\")\n    print(\"  - Text analysis (NLP)\")\n    print(\"  - Pattern recognition\")\n    print(\"  - Sequence similarity\")\n    print(\"  - Classification based on local features\")\n\n    demo_basic_ngram_encoding()\n    demo_different_n_values()\n    demo_stride_parameter()\n    demo_text_similarity()\n    demo_bundling_vs_chaining()\n    demo_character_ngrams()\n    demo_application_text_classification()\n\n    print(\"\\n\" + \"=\" * 70)\n    print(\"Demo Complete!\")\n    print(\"=\" * 70)\n    print(\"\\nNext steps:\")\n    print(\"  - See docs/theory/encoders.md for mathematical details\")\n    print(\"  - Run tests: pytest tests/test_encoders_sequence.py -k NGram\")\n    print(\"  - Try with different VSA models (FHRR, HRR, BSC)\")\n\n\nif __name__ == '__main__':\n    main()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}