{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# Gesture Recognition from Motion Trajectories\n\nTopics: Trajectory encoding, motion classification, time series, HCI\nTime: 15 minutes\nPrerequisites: 15_encoders_trajectory.py, 10_encoders_scalar.py\nRelated: 20_app_text_classification.py, 21_app_image_recognition.py\n\nThis example demonstrates practical gesture recognition using trajectory\nencoding and hyperdimensional computing. Learn how to classify motion\npatterns from continuous trajectories.\n\nKey concepts:\n- Trajectory encoding: Continuous paths in 2D/3D space\n- Temporal patterns: Motion sequences over time\n- Gesture classification: Similarity-based matching\n- Real-time processing: Efficient online recognition\n\nGesture recognition with HDC is fast, memory-efficient, and works well\nfor real-time applications on edge devices (wearables, smartphones, etc.).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import numpy as np\nfrom holovec import VSA\nfrom holovec.encoders import TrajectoryEncoder, FractionalPowerEncoder\nfrom holovec.retrieval import ItemStore\n\nprint(\"=\" * 70)\nprint(\"Gesture Recognition from Motion Trajectories\")\nprint(\"=\" * 70)\nprint()\n\n# Create model and encoder\nmodel = VSA.create('FHRR', dim=10000, seed=42)\n\n# Trajectory encoder for 2D motion\n# Use a single scalar encoder for all dimensions (x, y)\nscalar_encoder = FractionalPowerEncoder(model, min_val=-1, max_val=1, bandwidth=0.1, seed=42)\n\ntrajectory_encoder = TrajectoryEncoder(\n    model,\n    scalar_encoder=scalar_encoder,\n    n_dimensions=2,\n    seed=44\n)\n\nprint(f\"Model: {model.model_name}, dimension={model.dimension}\")\nprint(f\"Trajectory encoder: 2D motion, 20 time steps\")\nprint()\n\n# ============================================================================\n# Dataset: Simple 2D Gestures\n# ============================================================================\nprint(\"=\" * 70)\nprint(\"Dataset: Simple 2D Gesture Patterns\")\nprint(\"=\" * 70)\n\nnp.random.seed(42)\n\n# Define gesture generators\ndef create_circle(noise=0.0):\n    \"\"\"Circular motion (clockwise).\"\"\"\n    t = np.linspace(0, 2*np.pi, 20)\n    x = 0.5 * np.cos(t) + np.random.randn(20) * noise\n    y = 0.5 * np.sin(t) + np.random.randn(20) * noise\n    return np.column_stack([x, y])\n\ndef create_line_horizontal(noise=0.0):\n    \"\"\"Horizontal line (left to right).\"\"\"\n    t = np.linspace(-0.8, 0.8, 20)\n    x = t + np.random.randn(20) * noise\n    y = np.zeros(20) + np.random.randn(20) * noise\n    return np.column_stack([x, y])\n\ndef create_line_vertical(noise=0.0):\n    \"\"\"Vertical line (bottom to top).\"\"\"\n    t = np.linspace(-0.8, 0.8, 20)\n    x = np.zeros(20) + np.random.randn(20) * noise\n    y = t + np.random.randn(20) * noise\n    return np.column_stack([x, y])\n\ndef create_zigzag(noise=0.0):\n    \"\"\"Zigzag pattern.\"\"\"\n    t = np.linspace(0, 4*np.pi, 20)\n    x = np.linspace(-0.8, 0.8, 20) + np.random.randn(20) * noise\n    y = 0.3 * np.sin(3*t) + np.random.randn(20) * noise\n    return np.column_stack([x, y])\n\n# Generate training examples\nprint(\"\\nGenerating training gestures (4 classes, 5 examples each):\")\n\ntraining_data = []\nnoise_level = 0.05\n\n# Circle gestures\nfor i in range(5):\n    traj = create_circle(noise=noise_level)\n    training_data.append((traj, \"circle\"))\n\n# Horizontal lines\nfor i in range(5):\n    traj = create_line_horizontal(noise=noise_level)\n    training_data.append((traj, \"horizontal\"))\n\n# Vertical lines\nfor i in range(5):\n    traj = create_line_vertical(noise=noise_level)\n    training_data.append((traj, \"vertical\"))\n\n# Zigzags\nfor i in range(5):\n    traj = create_zigzag(noise=noise_level)\n    training_data.append((traj, \"zigzag\"))\n\nprint(f\"  circle:     5 examples\")\nprint(f\"  horizontal: 5 examples\")\nprint(f\"  vertical:   5 examples\")\nprint(f\"  zigzag:     5 examples\")\nprint(f\"\\nTotal: {len(training_data)} training gestures\")\n\n# ============================================================================\n# Training: Build Gesture Prototypes\n# ============================================================================\nprint(\"\\n\" + \"=\" * 70)\nprint(\"Training: Building Gesture Prototypes\")\nprint(\"=\" * 70)\n\n# Group by class\ngesture_classes = {}\nfor traj, label in training_data:\n    if label not in gesture_classes:\n        gesture_classes[label] = []\n    gesture_classes[label].append(traj)\n\n# Encode and bundle per class\ngesture_prototypes = {}\n\nprint(\"\\nEncoding gesture patterns:\")\nfor label, trajectories in gesture_classes.items():\n    encoded = [trajectory_encoder.encode(traj) for traj in trajectories]\n    prototype = model.bundle(encoded)\n    gesture_prototypes[label] = prototype\n    print(f\"  {label:12s}: {len(trajectories)} examples \u2192 prototype\")\n\nprint(f\"\\nGesture prototypes created: {len(gesture_prototypes)}\")\n\n# ============================================================================\n# Recognition: Test on New Gestures\n# ============================================================================\nprint(\"\\n\" + \"=\" * 70)\nprint(\"Recognition: Testing on New Gestures\")\nprint(\"=\" * 70)\n\n# Create test gestures with moderate noise\ntest_gestures = [\n    (create_circle(noise=0.08), \"circle\"),\n    (create_line_horizontal(noise=0.08), \"horizontal\"),\n    (create_line_vertical(noise=0.08), \"vertical\"),\n    (create_zigzag(noise=0.08), \"zigzag\"),\n]\n\nprint(\"\\nRecognizing test gestures:\")\nprint()\n\ncorrect = 0\nfor i, (traj, expected) in enumerate(test_gestures, 1):\n    # Encode test gesture\n    test_hv = trajectory_encoder.encode(traj)\n\n    # Find most similar prototype\n    best_label = None\n    best_sim = float('-inf')\n\n    for label, prototype in gesture_prototypes.items():\n        sim = float(model.similarity(test_hv, prototype))\n        if sim > best_sim:\n            best_sim = sim\n            best_label = label\n\n    is_correct = (best_label == expected)\n    correct += (1 if is_correct else 0)\n    marker = \"\u2713\" if is_correct else \"\u2717\"\n\n    print(f\"{i}. Gesture: {expected:12s}\")\n    print(f\"   Recognized: {best_label:12s} (similarity={best_sim:.3f}) {marker}\")\n    print()\n\naccuracy = correct / len(test_gestures)\nprint(f\"Accuracy: {correct}/{len(test_gestures)} = {accuracy:.1%}\")\n\n# ============================================================================\n# Analysis: Gesture Similarity\n# ============================================================================\nprint(\"\\n\" + \"=\" * 70)\nprint(\"Analysis: Gesture Confusion Matrix\")\nprint(\"=\" * 70)\n\nprint(\"\\nSimilarity between gesture prototypes:\")\nlabels = sorted(gesture_prototypes.keys())\n\n# Print header\nprint(f\"{'':12s}\", end=\"\")\nfor label in labels:\n    print(f\" {label:>10s}\", end=\"\")\nprint()\n\n# Print matrix\nfor label1 in labels:\n    print(f\"{label1:12s}\", end=\"\")\n    for label2 in labels:\n        sim = float(model.similarity(gesture_prototypes[label1],\n                                      gesture_prototypes[label2]))\n        print(f\" {sim:10.3f}\", end=\"\")\n    print()\n\nprint(\"\\nKey observation:\")\nprint(\"  - Diagonal = 1.0 (self-similarity)\")\nprint(\"  - Horizontal & vertical somewhat similar (both lines)\")\nprint(\"  - Circle & zigzag clearly distinct\")\n\n# ============================================================================\n# Real-Time Considerations\n# ============================================================================\nprint(\"\\n\" + \"=\" * 70)\nprint(\"Real-Time Gesture Recognition\")\nprint(\"=\" * 70)\n\nprint(\"\\n\u26a1 Real-time processing advantages:\")\nprint(\"  - Fast encoding: ~1ms for 20-point trajectory\")\nprint(\"  - Immediate classification: Single similarity computation\")\nprint(\"  - Memory efficient: Fixed-size hypervectors\")\nprint(\"  - Incremental: Can process partial gestures\")\nprint(\"  - No GPU required: Runs on CPU, microcontrollers\")\nprint()\n\nprint(\"Implementation tips:\")\nprint(\"  - Sample trajectory at fixed rate (e.g., 20 points/sec)\")\nprint(\"  - Normalize to [-1, 1] range before encoding\")\nprint(\"  - Use sliding window for continuous recognition\")\nprint(\"  - Threshold similarity for rejection (unknown gestures)\")\nprint(\"  - Retrain prototypes with user-specific data\")\nprint()\n\n# ============================================================================\n# Extension: Multi-User Recognition System\n# ============================================================================\nprint(\"=\" * 70)\nprint(\"Extension: Multi-User Gesture Library\")\nprint(\"=\" * 70)\n\n# Build gesture library with ItemStore\ngesture_library = ItemStore(model)\nfor label, prototype in gesture_prototypes.items():\n    gesture_library.add(label, prototype)\n\nprint(f\"\\nGesture library built with {len(gesture_prototypes)} gestures\")\n\n# Test with ambiguous gesture (partial circle)\nt = np.linspace(0, np.pi, 20)  # Half circle\npartial_x = 0.5 * np.cos(t) + np.random.randn(20) * 0.05\npartial_y = 0.5 * np.sin(t) + np.random.randn(20) * 0.05\npartial_circle = np.column_stack([partial_x, partial_y])\n\ntest_hv = trajectory_encoder.encode(partial_circle)\n\n# Query library\nresults = gesture_library.query(test_hv, k=4)\n\nprint(\"\\nTest gesture: partial circle (first half only)\")\nprint(\"\\nTop matches:\")\nfor i, (label, sim) in enumerate(results, 1):\n    print(f\"  {i}. {label:12s}: {sim:.3f}\")\n\nprint(\"\\nKey observation:\")\nprint(\"  - Partial gestures still match similar patterns\")\nprint(\"  - Can examine top-k for ambiguous cases\")\nprint(\"  - Threshold similarity to reject uncertain gestures\")\n\n# ============================================================================\n# Practical Considerations\n# ============================================================================\nprint(\"\\n\" + \"=\" * 70)\nprint(\"Practical Considerations\")\nprint(\"=\" * 70)\n\nprint(\"\\n\u2713 Advantages of HDC Gesture Recognition:\")\nprint(\"  - Fast: Real-time processing on edge devices\")\nprint(\"  - Efficient: Low memory and compute requirements\")\nprint(\"  - Robust: Tolerant to noise and variations\")\nprint(\"  - Adaptable: Easy to add new gesture classes\")\nprint(\"  - Interpretable: Similarity scores show confidence\")\nprint()\n\nprint(\"\u2717 Limitations:\")\nprint(\"  - Fixed length: Requires normalizing to fixed points\")\nprint(\"  - Simple patterns: Best for distinct gestures\")\nprint(\"  - No learning: Doesn't adapt like neural networks\")\nprint(\"  - Temporal detail: May miss fine-grained timing\")\nprint()\n\nprint(\"Best use cases:\")\nprint(\"  - Wearable devices (smartwatches, fitness trackers)\")\nprint(\"  - Smartphone gesture controls\")\nprint(\"  - Sign language recognition (simple gestures)\")\nprint(\"  - Robot control (motion commands)\")\nprint(\"  - VR/AR interaction (hand tracking)\")\nprint()\n\n# ============================================================================\n# Summary\n# ============================================================================\nprint(\"=\" * 70)\nprint(\"Summary: Gesture Recognition with HDC\")\nprint(\"=\" * 70)\nprint()\nprint(\"Complete workflow:\")\nprint(\"  1. Setup: Create model + TrajectoryEncoder\")\nprint(\"  2. Training: Encode trajectories + bundle per gesture\")\nprint(\"  3. Recognition: Encode test trajectory + find nearest prototype\")\nprint(\"  4. Deployment: Real-time recognition with similarity threshold\")\nprint()\nprint(\"Performance tips:\")\nprint(\"  - Normalize trajectories to consistent range\")\nprint(\"  - Use ~20-30 time steps for good temporal resolution\")\nprint(\"  - More training examples \u2192 better noise tolerance\")\nprint(\"  - Consider user-specific adaptation (personalization)\")\nprint()\nprint(\"Next steps:\")\nprint(\"  \u2192 Try with real accelerometer/gyroscope data\")\nprint(\"  \u2192 Extend to 3D trajectories (x, y, z)\")\nprint(\"  \u2192 Implement sliding window for continuous recognition\")\nprint(\"  \u2192 Combine with 25_app_integration_patterns.py for multimodal\")\nprint()\nprint(\"=\" * 70)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}