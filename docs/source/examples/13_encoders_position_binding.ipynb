{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# Position-Based Sequence Encoding\n\nTopics: PositionBindingEncoder, order sensitivity, sequence similarity\nTime: 15 minutes\nPrerequisites: 00_quickstart.py, 01_basic_operations.py\nRelated: 14_encoders_ngram.py, 15_encoders_trajectory.py\n\nThis example demonstrates the PositionBindingEncoder, which encodes sequences\nby binding each element to a unique position vector. This creates order-sensitive\nrepresentations where different arrangements of the same elements produce distinct\nhypervectors.\n\nKey concepts:\n- Position binding: bind(symbol_i, position_i) for each element\n- Order sensitivity: permutations are distinguishable\n- Sequence similarity: shared prefixes increase similarity\n- Reversible encoding: can decode to recover symbols\n\nThe PositionBindingEncoder is fundamental for text processing, time series,\nand any ordered data where position matters.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from holovec import VSA\nfrom holovec.encoders import PositionBindingEncoder\n\nprint(\"=\" * 70)\nprint(\"Position-Based Sequence Encoding\")\nprint(\"=\" * 70)\nprint()\n\n# ============================================================================\n# Demo 1: Basic Usage\n# ============================================================================\nprint(\"=\" * 70)\nprint(\"Demo 1: Basic PositionBindingEncoder Usage\")\nprint(\"=\" * 70)\n\n# Create model\nmodel = VSA.create('MAP', dim=5000, seed=42)\n\n# Create encoder\nencoder = PositionBindingEncoder(model, seed=42)\n\nprint(f\"\\nEncoder: {encoder}\")\nprint(f\"Reversible: {encoder.is_reversible}\")\nprint(f\"Compatible models: {encoder.compatible_models}\")\n\n# Encode some sequences\nsequences = [\n    ['hello', 'world'],\n    ['hello', 'world', '!'],\n    ['goodbye', 'world'],\n    ['world', 'hello']  # Reversed order\n]\n\nprint(\"\\nEncoding sequences:\")\nencoded = []\nfor seq in sequences:\n    hv = encoder.encode(seq)\n    encoded.append(hv)\n    print(f\"  {seq} \u2192 HV shape: {hv.shape}\")\n\n# Check similarities\nprint(\"\\nSimilarity Matrix:\")\nfor i, seq1 in enumerate(sequences):\n    similarities = []\n    for j, seq2 in enumerate(sequences):\n        sim = float(model.similarity(encoded[i], encoded[j]))\n        similarities.append(sim)\n    seq_str = str(seq1)[:30].ljust(30)\n    sims_str = \"  \".join(f\"{s:5.3f}\" for s in similarities)\n    print(f\"{seq_str} | {sims_str}\")\n\n# Test decoding\nprint(\"\\nDecoding test (first 3 positions):\")\nfor i, seq in enumerate(sequences[:2]):  # Only decode first 2\n    decoded = encoder.decode(encoded[i], max_positions=5, threshold=0.2)\n    print(f\"  Original: {seq}\")\n    print(f\"  Decoded:  {decoded}\\n\")\n\nprint(\"Key observations:\")\nprint(\"  - Identical sequences have similarity \u2248 1.0\")\nprint(\"  - Shared prefix increases similarity\")\nprint(\"  - Different order creates different encodings\")\nprint(\"  - Decoding recovers first few symbols accurately\")\n\n# ============================================================================\n# Demo 2: Order Sensitivity\n# ============================================================================\nprint(\"\\n\" + \"=\" * 70)\nprint(\"Demo 2: Order Sensitivity\")\nprint(\"=\" * 70)\n\n# Test order sensitivity\noriginal = ['a', 'b', 'c', 'd']\npermutations = [\n    (['a', 'b', 'c', 'd'], \"Original\"),\n    (['d', 'c', 'b', 'a'], \"Reversed\"),\n    (['b', 'c', 'd', 'a'], \"Rotated 1\"),\n    (['c', 'd', 'a', 'b'], \"Rotated 2\"),\n]\n\nprint(\"\\nTesting order sensitivity:\")\nref_hv = encoder.encode(original)\n\nprint(f\"Reference: {original}\")\nprint(\"\\nSequence              | Similarity | Description\")\nprint(\"-\" * 60)\n\nfor seq, desc in permutations:\n    hv = encoder.encode(seq)\n    sim = float(model.similarity(ref_hv, hv))\n    seq_str = str(seq).ljust(20)\n    print(f\"{seq_str} | {sim:10.3f} | {desc}\")\n\nprint(\"\\nKey observation:\")\nprint(\"  - Different orders produce distinct encodings\")\nprint(\"  - Even rotations are clearly distinguishable\")\n\n# ============================================================================\n# Demo 3: Sequence Similarity\n# ============================================================================\nprint(\"\\n\" + \"=\" * 70)\nprint(\"Demo 3: Sequence Similarity and Prefix Matching\")\nprint(\"=\" * 70)\n\n# Test prefix matching\nreference = ['the', 'quick', 'brown', 'fox', 'jumps']\nvariants = [\n    (['the', 'quick', 'brown', 'fox', 'jumps'], \"Identical\"),\n    (['the', 'quick', 'brown', 'fox'], \"Prefix (4/5)\"),\n    (['the', 'quick', 'brown'], \"Prefix (3/5)\"),\n    (['the', 'quick'], \"Prefix (2/5)\"),\n    (['the'], \"Prefix (1/5)\"),\n    (['the', 'slow', 'brown', 'fox', 'walks'], \"1 match only\"),\n    (['a', 'completely', 'different', 'sentence'], \"No match\"),\n]\n\nprint(f\"\\nReference: {reference}\")\nprint(\"\\nSequence                                | Similarity | Shared\")\nprint(\"-\" * 70)\n\nref_hv = encoder.encode(reference)\n\nfor seq, desc in variants:\n    hv = encoder.encode(seq)\n    sim = float(model.similarity(ref_hv, hv))\n    seq_str = str(seq)[:40].ljust(40)\n    print(f\"{seq_str} | {sim:10.3f} | {desc}\")\n\nprint(\"\\nKey observations:\")\nprint(\"  - Longer shared prefix \u2192 higher similarity\")\nprint(\"  - Similarity degrades gracefully with differences\")\nprint(\"  - Enables approximate sequence matching\")\n\n# ============================================================================\n# Summary\n# ============================================================================\nprint(\"\\n\" + \"=\" * 70)\nprint(\"Summary: PositionBindingEncoder Key Takeaways\")\nprint(\"=\" * 70)\nprint()\nprint(\"\u2713 Order-sensitive: Different arrangements are distinguishable\")\nprint(\"\u2713 Prefix similarity: Shared prefixes increase similarity\")\nprint(\"\u2713 Reversible: Can decode to recover original symbols\")\nprint(\"\u2713 Foundation for text: Used in n-gram and language models\")\nprint(\"\u2713 Works with all models: Compatible with MAP, FHRR, HRR, BSC, BSDC\")\nprint()\nprint(\"Use cases:\")\nprint(\"  - Text processing: words in sentences\")\nprint(\"  - Time series: events in temporal order\")\nprint(\"  - Structured data: ordered records\")\nprint(\"  - Sequences: any data where position matters\")\nprint()\nprint(\"Next steps:\")\nprint(\"  \u2192 14_encoders_ngram.py - N-gram text encoding\")\nprint(\"  \u2192 15_encoders_trajectory.py - Continuous sequences\")\nprint(\"  \u2192 20_app_text_classification.py - Apply to real text data\")\nprint()\nprint(\"=\" * 70)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}