{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# Working Memory with Cleanup Strategies\n\nTopics: Working memory, cleanup, resonator, factorization, noisy retrieval\nTime: 20 minutes\nPrerequisites: 23_app_symbolic_reasoning.py, 26_retrieval_basics.py\nRelated: 27_cleanup_strategies.py, 28_factorization_methods.py\n\nThis example demonstrates working memory systems using hyperdimensional\ncomputing, with a focus on cleanup strategies for retrieving information\nfrom noisy bundled representations.\n\nKey concepts:\n- Working memory: Bundled representation of active information\n- Cleanup: Recovering clean symbols from noisy hypervectors\n- BruteForce cleanup: Exhaustive nearest-neighbor search\n- Resonator cleanup: Iterative refinement using codebook\n- Multi-factor unbinding: Decompose bundled representations\n- Noise tolerance: Graceful degradation with increasing bundle size\n\nWorking memory in HDC mimics human working memory: limited capacity,\ndistributed representation, and content-addressable retrieval.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from holovec import VSA\nfrom holovec.retrieval import ItemStore, Codebook\nfrom holovec.utils.cleanup import BruteForceCleanup, ResonatorCleanup\n\nprint(\"=\" * 70)\nprint(\"Working Memory with Cleanup Strategies\")\nprint(\"=\" * 70)\nprint()\n\n# Create model\nmodel = VSA.create('FHRR', dim=10000, seed=42)\n\n# ============================================================================\n# Demo 1: Basic Working Memory - Bundled Active Items\n# ============================================================================\nprint(\"=\" * 70)\nprint(\"Demo 1: Basic Working Memory\")\nprint(\"=\" * 70)\n\n# Simulate working memory with 5 active items\nprint(\"\\nEncoding 5 active items in working memory:\")\n\nitems = {}\nitems[\"book\"] = model.random(seed=100)\nitems[\"pen\"] = model.random(seed=101)\nitems[\"coffee\"] = model.random(seed=102)\nitems[\"phone\"] = model.random(seed=103)\nitems[\"keys\"] = model.random(seed=104)\n\nprint(\"  Items: book, pen, coffee, phone, keys\")\n\n# Bundle into working memory\nworking_memory = model.bundle([items[\"book\"], items[\"pen\"], items[\"coffee\"],\n                                items[\"phone\"], items[\"keys\"]])\n\nprint(\"\\nWorking memory: bundled all 5 items\")\n\n# Query: Is \"book\" in working memory?\nprint(\"\\n\" + \"=\" * 70)\nprint(\"Query: Check if items are in working memory\")\nprint(\"=\" * 70)\n\nprint(\"\\nSimilarity to active items:\")\nfor name, vec in items.items():\n    sim = float(model.similarity(working_memory, vec))\n    print(f\"  {name:10s}: {sim:.3f}\")\n\n# Check items NOT in working memory\nprint(\"\\nSimilarity to inactive items:\")\nlaptop = model.random(seed=200)\nmouse = model.random(seed=201)\nprint(f\"  laptop:     {float(model.similarity(working_memory, laptop)):.3f}\")\nprint(f\"  mouse:      {float(model.similarity(working_memory, mouse)):.3f}\")\n\nprint(\"\\nKey observation:\")\nprint(\"  - Active items have high similarity (~0.45)\")\nprint(\"  - Inactive items have low similarity (~0)\")\nprint(\"  - Working memory acts as distributed content-addressable store\")\n\n# ============================================================================\n# Demo 2: Cleanup with BruteForce Strategy\n# ============================================================================\nprint(\"\\n\" + \"=\" * 70)\nprint(\"Demo 2: BruteForce Cleanup Strategy\")\nprint(\"=\" * 70)\n\n# Create noisy query by adding noise\nprint(\"\\nSimulating noisy query:\")\nprint(\"  Original: book\")\n\n# Add noise to book vector\nnoise = model.random(seed=999)\nnoisy_book = model.bundle([items[\"book\"], noise])  # 50% book, 50% noise\n\nprint(f\"  Similarity noisy_book \u2192 book: {float(model.similarity(noisy_book, items['book'])):.3f}\")\nprint(f\"  (Pure book would be 1.000, random would be ~0)\")\n\n# Create codebook for cleanup\ncodebook = Codebook(items, backend=model.backend)\n\n# BruteForce cleanup: find nearest neighbor\nprint(\"\\n\" + \"=\" * 70)\nprint(\"Cleanup with BruteForce (nearest neighbor)\")\nprint(\"=\" * 70)\n\ncleanup_bf = BruteForceCleanup()\nlabels, sims = cleanup_bf.factorize(noisy_book, items, model, n_factors=3)\n\nprint(\"\\nTop 3 matches:\")\nfor i, (label, sim) in enumerate(zip(labels, sims), 1):\n    marker = \"  \u2190 Correct!\" if label == \"book\" else \"\"\n    print(f\"  {i}. {label:10s}: {sim:.3f}{marker}\")\n\nprint(\"\\nKey observation:\")\nprint(\"  - BruteForce finds nearest neighbor via exhaustive search\")\nprint(\"  - Correct even with significant noise\")\nprint(\"  - Fast for small codebooks, O(N) complexity\")\n\n# ============================================================================\n# Demo 3: Resonator Cleanup Strategy\n# ============================================================================\nprint(\"\\n\" + \"=\" * 70)\nprint(\"Demo 3: Resonator Cleanup Strategy\")\nprint(\"=\" * 70)\n\n# Resonator uses iterative refinement\nprint(\"\\nResonator cleanup (iterative refinement):\")\n\ncleanup_res = ResonatorCleanup()\n\n# Create moderately noisy query\nnoise_moderate = model.random(seed=888)\nnoisy_pen = model.bundle([items[\"pen\"], items[\"pen\"], items[\"pen\"], noise_moderate])  # 75% pen\n\nprint(f\"  Starting similarity to pen: {float(model.similarity(noisy_pen, items['pen'])):.3f}\")\n\nlabels_res, sims_res = cleanup_res.factorize(noisy_pen, items, model, n_factors=1,\n                                              max_iterations=10, threshold=0.99)\n\nprint(f\"\\nResonator result:\")\nprint(f\"  Best match: {labels_res[0]} (similarity={sims_res[0]:.3f})\")\n\nprint(\"\\nKey observation:\")\nprint(\"  - Resonator iteratively refines noisy input\")\nprint(\"  - Uses codebook to project onto valid subspace\")\nprint(\"  - More robust to noise than single nearest-neighbor\")\n\n# ============================================================================\n# Demo 4: Multi-Factor Unbinding - Decomposing Bundled Representations\n# ============================================================================\nprint(\"\\n\" + \"=\" * 70)\nprint(\"Demo 4: Multi-Factor Unbinding\")\nprint(\"=\" * 70)\n\n# Create bundle of multiple items\nprint(\"\\nBundling 3 items:\")\nitem1 = items[\"book\"]\nitem2 = items[\"coffee\"]\nitem3 = items[\"phone\"]\n\nbundle = model.bundle([item1, item2, item3])\n\nprint(\"  Bundle: book \u2295 coffee \u2295 phone\")\n\n# Factorize to recover all items\nprint(\"\\n\" + \"=\" * 70)\nprint(\"Factorizing bundle to recover all items:\")\nprint(\"=\" * 70)\n\nlabels_fact, sims_fact = cleanup_bf.factorize(bundle, items, model, n_factors=5)\n\nprint(\"\\nRecovered factors:\")\nfor i, (label, sim) in enumerate(zip(labels_fact, sims_fact), 1):\n    in_bundle = \"\u2713\" if label in [\"book\", \"coffee\", \"phone\"] else \"\u2717\"\n    print(f\"  {i}. {label:10s}: {sim:.3f}  [{in_bundle}]\")\n\nprint(\"\\nKey observation:\")\nprint(\"  - Factorization recovers multiple bundled items\")\nprint(\"  - Top factors are the original bundled items\")\nprint(\"  - Similarity degrades but items still identifiable\")\n\n# ============================================================================\n# Demo 5: Capacity Limits - How Many Items Can Working Memory Hold?\n# ============================================================================\nprint(\"\\n\" + \"=\" * 70)\nprint(\"Demo 5: Working Memory Capacity Limits\")\nprint(\"=\" * 70)\n\n# Test with increasing bundle sizes\nprint(\"\\nTesting working memory capacity:\")\n\n# Create larger item set\nlarge_items = {}\nfor i in range(20):\n    large_items[f\"item_{i}\"] = model.random(seed=300 + i)\n\n# Test different bundle sizes\nbundle_sizes = [3, 5, 7, 10, 15]\n\nprint(\"\\nRecovery accuracy vs. bundle size:\")\nprint(\"Size | Top-1 | Top-3 | Top-5\")\nprint(\"-\" * 40)\n\nfor size in bundle_sizes:\n    # Bundle first 'size' items\n    bundled_items = [large_items[f\"item_{i}\"] for i in range(size)]\n    bundle_test = model.bundle(bundled_items)\n\n    # Factorize to recover\n    recovered, _ = cleanup_bf.factorize(bundle_test, large_items, model, n_factors=5)\n\n    # Count correct in top-k\n    correct_labels = {f\"item_{i}\" for i in range(size)}\n    top1_correct = 1 if recovered[0] in correct_labels else 0\n    top3_correct = sum(1 for r in recovered[:3] if r in correct_labels) / min(3, size)\n    top5_correct = sum(1 for r in recovered[:5] if r in correct_labels) / min(5, size)\n\n    print(f\"  {size:2d} | {top1_correct:5.2f} | {top3_correct:5.2f} | {top5_correct:5.2f}\")\n\nprint(\"\\nKey observation:\")\nprint(\"  - Accuracy degrades with bundle size\")\nprint(\"  - Working memory capacity: ~5-7 items (like human WM!)\")\nprint(\"  - Distributed representation naturally limits capacity\")\n\n# ============================================================================\n# Demo 6: Structured Working Memory - Role-Filler Binding\n# ============================================================================\nprint(\"\\n\" + \"=\" * 70)\nprint(\"Demo 6: Structured Working Memory\")\nprint(\"=\" * 70)\n\n# Working memory with structure: current task context\nprint(\"\\nEncoding task context:\")\nprint(\"  Task: 'Write email to Bob about meeting'\")\n\n# Define roles\ntask_type = model.random(seed=400)\nrecipient = model.random(seed=401)\ntopic = model.random(seed=402)\n\n# Define fillers\nwrite_email = model.random(seed=500)\nbob_entity = model.random(seed=501)\nmeeting_topic = model.random(seed=502)\n\n# Create structured working memory\nwm_structure = model.bundle([\n    model.bind(task_type, write_email),\n    model.bind(recipient, bob_entity),\n    model.bind(topic, meeting_topic)\n])\n\nprint(\"\\nRole-filler bindings in working memory:\")\nprint(\"  task_type \u2297 write_email\")\nprint(\"  recipient \u2297 bob\")\nprint(\"  topic \u2297 meeting\")\n\n# Query roles\nprint(\"\\n\" + \"=\" * 70)\nprint(\"Querying working memory by role:\")\nprint(\"=\" * 70)\n\n# What is the task type?\ntask_query = model.unbind(wm_structure, task_type)\nprint(f\"\\nWhat is the task?\")\nprint(f\"  Similarity to write_email: {float(model.similarity(task_query, write_email)):.3f}  \u2190 Match\")\n\n# Who is the recipient?\nrecip_query = model.unbind(wm_structure, recipient)\nprint(f\"\\nWho is the recipient?\")\nprint(f\"  Similarity to bob: {float(model.similarity(recip_query, bob_entity)):.3f}  \u2190 Match\")\n\n# What is the topic?\ntopic_query = model.unbind(wm_structure, topic)\nprint(f\"\\nWhat is the topic?\")\nprint(f\"  Similarity to meeting: {float(model.similarity(topic_query, meeting_topic)):.3f}  \u2190 Match\")\n\nprint(\"\\nKey observation:\")\nprint(\"  - Structured working memory supports role-based queries\")\nprint(\"  - Combines bundling (multiple bindings) with binding (role-filler)\")\nprint(\"  - Enables cognitive architectures with WM component\")\n\n# ============================================================================\n# Demo 7: Working Memory with ItemStore\n# ============================================================================\nprint(\"\\n\" + \"=\" * 70)\nprint(\"Demo 7: Working Memory with ItemStore\")\nprint(\"=\" * 70)\n\n# Create item store for semantic memory (long-term)\nprint(\"\\nBuilding semantic memory (long-term store):\")\n\nsemantic_memory = ItemStore(model)\nsemantic_memory.add(\"alice\", model.random(seed=600))\nsemantic_memory.add(\"bob\", model.random(seed=601))\nsemantic_memory.add(\"charlie\", model.random(seed=602))\nsemantic_memory.add(\"email\", model.random(seed=603))\nsemantic_memory.add(\"meeting\", model.random(seed=604))\nsemantic_memory.add(\"document\", model.random(seed=605))\n\nprint(\"  Stored: alice, bob, charlie, email, meeting, document\")\n\n# Working memory holds current focus\nprint(\"\\nWorking memory (current focus):\")\nwm_current = model.bundle([\n    semantic_memory.codebook._items[\"bob\"],\n    semantic_memory.codebook._items[\"meeting\"]\n])\n\nprint(\"  Active: bob, meeting\")\n\n# Query: What's in working memory?\nprint(\"\\n\" + \"=\" * 70)\nprint(\"Retrieving from working memory:\")\nprint(\"=\" * 70)\n\n# Use ItemStore to query working memory\nresults = semantic_memory.query(wm_current, k=6)\n\nprint(\"\\nTop matches in semantic memory:\")\nfor i, (label, sim) in enumerate(results, 1):\n    in_wm = \"\u2713\" if label in [\"bob\", \"meeting\"] else \" \"\n    print(f\"  {i}. {label:10s}: {sim:.3f}  [{in_wm}]\")\n\nprint(\"\\nKey observation:\")\nprint(\"  - ItemStore enables fast retrieval from semantic memory\")\nprint(\"  - Working memory acts as query to semantic memory\")\nprint(\"  - Models interaction between WM and long-term memory\")\n\n# ============================================================================\n# Summary\n# ============================================================================\nprint(\"\\n\" + \"=\" * 70)\nprint(\"Summary: Working Memory Key Takeaways\")\nprint(\"=\" * 70)\nprint()\nprint(\"\u2713 Working memory: Bundled representation of active items\")\nprint(\"\u2713 Content-addressable: Query by similarity, not address\")\nprint(\"\u2713 Cleanup strategies: Recover clean symbols from noise\")\nprint(\"\u2713 BruteForce: Fast O(N) nearest-neighbor search\")\nprint(\"\u2713 Resonator: Iterative refinement for robustness\")\nprint(\"\u2713 Multi-factor unbinding: Decompose bundled items\")\nprint(\"\u2713 Capacity limits: ~5-7 items (mirrors human WM!)\")\nprint()\nprint(\"Cleanup strategy comparison:\")\nprint(\"  BruteForce:\")\nprint(\"    - Fast for small codebooks (O(N))\")\nprint(\"    - Single nearest-neighbor lookup\")\nprint(\"    - Good for clean or moderately noisy queries\")\nprint()\nprint(\"  Resonator:\")\nprint(\"    - Iterative refinement (O(k*N), k iterations)\")\nprint(\"    - Projects onto valid subspace using codebook\")\nprint(\"    - Robust to higher noise levels\")\nprint(\"    - Can recover from very corrupted inputs\")\nprint()\nprint(\"Working memory applications:\")\nprint(\"  - Cognitive architectures: Active information maintenance\")\nprint(\"  - Attention mechanisms: Focus on relevant items\")\nprint(\"  - Task context: Maintain current goals and parameters\")\nprint(\"  - Short-term buffers: Temporary storage before consolidation\")\nprint()\nprint(\"Next steps:\")\nprint(\"  \u2192 27_cleanup_strategies.py - Detailed cleanup comparison\")\nprint(\"  \u2192 28_factorization_methods.py - Advanced unbinding techniques\")\nprint(\"  \u2192 25_app_integration_patterns.py - Integrate WM in larger systems\")\nprint()\nprint(\"=\" * 70)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}