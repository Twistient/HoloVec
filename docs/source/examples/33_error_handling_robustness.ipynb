{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# Error Handling and Robustness\n\nTopics: Noise tolerance, error propagation, fault recovery, graceful degradation\nTime: 15 minutes\nPrerequisites: 01_basic_operations.py, 27_cleanup_strategies.py\nRelated: 32_distributed_representations.py, 31_performance_benchmarks.py\n\nThis example demonstrates how hyperdimensional computing gracefully handles\nerrors, noise, and partial corruption - one of HDC's key strengths.\n\nKey concepts:\n- Noise tolerance: Similar vectors remain similar under corruption\n- Error propagation: How errors spread through operations\n- Graceful degradation: Performance decreases smoothly\n- Recovery strategies: Cleanup, redundancy, dimension tuning\n- Practical robustness: Real-world sensor noise, transmission errors\n\nHDC is inherently robust, making it ideal for edge devices, noisy sensors,\nand fault-tolerant systems.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import numpy as np\nfrom holovec import VSA\nfrom holovec.utils.cleanup import BruteForceCleanup, ResonatorCleanup\n\nprint(\"=\" * 70)\nprint(\"Error Handling and Robustness\")\nprint(\"=\" * 70)\nprint()\n\n# ============================================================================\n# Demo 1: Noise Tolerance Basics\n# ============================================================================\nprint(\"=\" * 70)\nprint(\"Demo 1: Noise Tolerance - Bit Flip Robustness\")\nprint(\"=\" * 70)\n\nmodel = VSA.create('MAP', dim=10000, seed=42)\n\nprint(f\"\\nModel: {model.model_name}\")\nprint(f\"Dimension: {model.dimension}\")\nprint()\n\n# Create a vector and add noise by flipping bits\noriginal = model.random(seed=1)\n\nflip_percentages = [0, 1, 5, 10, 20, 30, 40, 50]\n\nprint(f\"{'Noise %':<12s} {'Similarity':<15s} {'Still Recognizable?':<20s}\")\nprint(\"-\" * 55)\n\nfor flip_pct in flip_percentages:\n    # Create noisy version by flipping random bits\n    noisy = original.copy() if hasattr(original, 'copy') else original\n\n    if flip_pct > 0:\n        # Simulate bit flips by bundling with random noise\n        noise_strength = flip_pct / 100.0\n        noise = model.random(seed=999)\n\n        # Weighted bundle: more original, less noise\n        # For MAP: approximate via bundling\n        noisy = model.bundle([original] * int(100 - flip_pct) + [noise] * int(flip_pct))\n\n    sim = float(model.similarity(original, noisy))\n    recognizable = \"Yes\" if sim > 0.7 else \"Marginal\" if sim > 0.4 else \"No\"\n\n    print(f\"{flip_pct:10d}%   {sim:13.3f}   {recognizable:<20s}\")\n\nprint(\"\\nKey insight:\")\nprint(\"  - Tolerates up to ~20% corruption while maintaining similarity\")\nprint(\"  - Degrades gracefully (no sudden failure)\")\nprint(\"  - Higher dimension increases noise tolerance\")\n\n# ============================================================================\n# Demo 2: Error Propagation Through Operations\n# ============================================================================\nprint(\"\\n\" + \"=\" * 70)\nprint(\"Demo 2: Error Propagation Analysis\")\nprint(\"=\" * 70)\n\nmodel = VSA.create('MAP', dim=10000, seed=42)\n\n# Clean vectors\nA_clean = model.random(seed=1)\nB_clean = model.random(seed=2)\n\n# Add 10% noise to each\nnp.random.seed(42)\nnoise_A = model.random(seed=100)\nnoise_B = model.random(seed=101)\n\nA_noisy = model.bundle([A_clean] * 9 + [noise_A])\nB_noisy = model.bundle([B_clean] * 9 + [noise_B])\n\nprint(f\"\\nInput noise: ~10% corruption on each vector\")\nprint()\n\n# Test different operations\nprint(f\"{'Operation':<20s} {'Clean Result':<15s} {'Noisy Result':<15s} {'Degradation':<12s}\")\nprint(\"-\" * 70)\n\n# Binding\nAB_clean = model.bind(A_clean, B_clean)\nAB_noisy = model.bind(A_noisy, B_noisy)\nsim_bind = float(model.similarity(AB_clean, AB_noisy))\nprint(f\"{'Bind (A * B)':<20s} {1.0:13.3f}   {sim_bind:13.3f}   {1.0 - sim_bind:10.3f}\")\n\n# Bundling\nbundle_clean = model.bundle([A_clean, B_clean])\nbundle_noisy = model.bundle([A_noisy, B_noisy])\nsim_bundle = float(model.similarity(bundle_clean, bundle_noisy))\nprint(f\"{'Bundle (A + B)':<20s} {1.0:13.3f}   {sim_bundle:13.3f}   {1.0 - sim_bundle:10.3f}\")\n\n# Permutation (if available)\ntry:\n    perm_clean = model.permute(A_clean)\n    perm_noisy = model.permute(A_noisy)\n    sim_perm = float(model.similarity(perm_clean, perm_noisy))\n    print(f\"{'Permute (\u03c1(A))':<20s} {1.0:13.3f}   {sim_perm:13.3f}   {1.0 - sim_perm:10.3f}\")\nexcept:\n    pass\n\n# Unbinding\nrecovered_clean = model.unbind(AB_clean, A_clean)\nrecovered_noisy = model.unbind(AB_noisy, A_noisy)\nsim_unbind_clean = float(model.similarity(recovered_clean, B_clean))\nsim_unbind_noisy = float(model.similarity(recovered_noisy, B_clean))\nprint(f\"{'Unbind (AB / A)':<20s} {sim_unbind_clean:13.3f}   {sim_unbind_noisy:13.3f}   {sim_unbind_clean - sim_unbind_noisy:10.3f}\")\n\nprint(\"\\nObservations:\")\nprint(\"  - Error propagates but doesn't amplify catastrophically\")\nprint(\"  - Bundling is most robust (averaging effect)\")\nprint(\"  - Binding maintains reasonable similarity\")\nprint(\"  - Operations exhibit graceful degradation\")\n\n# ============================================================================\n# Demo 3: Dimension vs Noise Tolerance\n# ============================================================================\nprint(\"\\n\" + \"=\" * 70)\nprint(\"Demo 3: Dimension Effect on Noise Tolerance\")\nprint(\"=\" * 70)\n\nnoise_level = 0.2  # 20% noise\n\nprint(f\"\\nNoise level: {noise_level * 100:.0f}%\")\nprint()\n\ndimensions = [1000, 5000, 10000, 20000]\n\nprint(f\"{'Dimension':<12s} {'Similarity':<15s} {'Recognizable?':<15s}\")\nprint(\"-\" * 50)\n\nfor dim in dimensions:\n    model = VSA.create('MAP', dim=dim, seed=42)\n\n    original = model.random(seed=1)\n    noise = model.random(seed=999)\n\n    # Add noise\n    noisy = model.bundle([original] * int(100 * (1 - noise_level)) +\n                          [noise] * int(100 * noise_level))\n\n    sim = float(model.similarity(original, noisy))\n    recognizable = \"Yes\" if sim > 0.7 else \"Marginal\" if sim > 0.4 else \"No\"\n\n    print(f\"{dim:<12d} {sim:13.3f}   {recognizable:<15s}\")\n\nprint(\"\\nKey insight:\")\nprint(\"  - Higher dimension = better noise tolerance\")\nprint(\"  - Diminishing returns above ~10,000 dimensions\")\nprint(\"  - Choose dimension based on expected noise level\")\n\n# ============================================================================\n# Demo 4: Sensor Noise Simulation\n# ============================================================================\nprint(\"\\n\" + \"=\" * 70)\nprint(\"Demo 4: Realistic Sensor Noise Scenario\")\nprint(\"=\" * 70)\n\nfrom holovec.encoders import FractionalPowerEncoder\n\nmodel = VSA.create('FHRR', dim=10000, seed=42)\n\n# Temperature sensor with noise\nencoder = FractionalPowerEncoder(model, min_val=0, max_val=100, bandwidth=0.1, seed=42)\n\ntrue_temp = 37.5\nnoise_std = 0.5  # \u00b10.5\u00b0C sensor noise\n\nprint(f\"\\nTrue temperature: {true_temp}\u00b0C\")\nprint(f\"Sensor noise: \u00b1{noise_std}\u00b0C (std dev)\")\nprint()\n\n# Simulate 10 noisy readings\nnp.random.seed(42)\nreadings = true_temp + np.random.randn(10) * noise_std\n\nprint(\"Noisy readings:\")\nfor i, reading in enumerate(readings, 1):\n    print(f\"  {i:2d}. {reading:6.2f}\u00b0C (error: {reading - true_temp:+.2f}\u00b0C)\")\n\n# Encode each reading\nencoded_readings = [encoder.encode(r) for r in readings]\n\n# Average the encoded vectors (noise reduction through bundling)\naveraged = model.bundle(encoded_readings)\n\n# Decode\ndecoded_temp = encoder.decode(averaged)\n\nprint(f\"\\nDecoded from averaged HVs: {decoded_temp:.2f}\u00b0C\")\nprint(f\"Error from true value: {abs(decoded_temp - true_temp):.2f}\u00b0C\")\nprint(f\"Improvement: {np.mean([abs(r - true_temp) for r in readings]) / abs(decoded_temp - true_temp):.1f}x\")\n\nprint(\"\\nBenefit:\")\nprint(\"  - Bundling multiple noisy measurements reduces error\")\nprint(\"  - HDC naturally implements sensor fusion\")\nprint(\"  - Robust to individual sensor failures\")\n\n# ============================================================================\n# Demo 5: Transmission Error Recovery\n# ============================================================================\nprint(\"\\n\" + \"=\" * 70)\nprint(\"Demo 5: Communication Channel Errors\")\nprint(\"=\" * 70)\n\nmodel = VSA.create('MAP', dim=10000, seed=42)\n\n# Create a codebook\ncodebook = {\n    \"message_A\": model.random(seed=1),\n    \"message_B\": model.random(seed=2),\n    \"message_C\": model.random(seed=3),\n    \"message_D\": model.random(seed=4),\n    \"message_E\": model.random(seed=5),\n}\n\ncleanup = BruteForceCleanup()\n\nprint(\"\\nSimulating transmission errors:\")\nprint()\n\nerror_rates = [0, 0.05, 0.10, 0.15, 0.20]\n\nprint(f\"{'Error Rate':<12s} {'Correct ID Rate':<18s} {'Avg Rank':<10s}\")\nprint(\"-\" * 48)\n\nfor error_rate in error_rates:\n    correct = 0\n    ranks = []\n\n    for msg_name, msg_hv in codebook.items():\n        # Simulate transmission error\n        if error_rate > 0:\n            noise = model.random(seed=999)\n            received = model.bundle([msg_hv] * int(100 * (1 - error_rate)) +\n                                    [noise] * int(100 * error_rate))\n        else:\n            received = msg_hv\n\n        # Try to identify using cleanup\n        labels, sims = cleanup.factorize(received, codebook, model, n_factors=1)\n\n        if labels[0] == msg_name:\n            correct += 1\n\n        # Find rank of correct message\n        all_sims = [(name, float(model.similarity(received, hv)))\n                    for name, hv in codebook.items()]\n        all_sims.sort(key=lambda x: x[1], reverse=True)\n        rank = next(i for i, (name, _) in enumerate(all_sims, 1) if name == msg_name)\n        ranks.append(rank)\n\n    accuracy = correct / len(codebook)\n    avg_rank = np.mean(ranks)\n\n    print(f\"{error_rate * 100:10.0f}%   {accuracy:16.1%}   {avg_rank:8.1f}\")\n\nprint(\"\\nInsight:\")\nprint(\"  - Tolerates up to 15% transmission error with cleanup\")\nprint(\"  - Even with errors, correct message often in top-3\")\nprint(\"  - Error correction codes can further improve robustness\")\n\n# ============================================================================\n# Demo 6: Recovery Strategies\n# ============================================================================\nprint(\"\\n\" + \"=\" * 70)\nprint(\"Demo 6: Error Recovery with Cleanup\")\nprint(\"=\" * 70)\n\nmodel = VSA.create('MAP', dim=10000, seed=42)\n\ncodebook = {f\"item_{i}\": model.random(seed=100+i) for i in range(50)}\n\n# Heavily corrupted query\ntarget_key = \"item_25\"\ntarget = codebook[target_key]\n\n# Add 30% noise\nnoise = model.random(seed=999)\ncorrupted = model.bundle([target] * 7 + [noise] * 3)\n\nprint(f\"\\nTarget: {target_key}\")\nprint(f\"Corruption: 30% noise\")\nprint()\n\n# Without cleanup\nsims_no_cleanup = [(key, float(model.similarity(corrupted, hv)))\n                    for key, hv in codebook.items()]\nsims_no_cleanup.sort(key=lambda x: x[1], reverse=True)\n\nprint(\"Top-5 matches WITHOUT cleanup:\")\nfor i, (key, sim) in enumerate(sims_no_cleanup[:5], 1):\n    marker = \" \u2190 Target!\" if key == target_key else \"\"\n    print(f\"  {i}. {key:12s}: {sim:.3f}{marker}\")\n\n# With BruteForce cleanup\nbf_cleanup = BruteForceCleanup()\nlabels_bf, sims_bf = bf_cleanup.factorize(corrupted, codebook, model, n_factors=5)\n\nprint(\"\\nTop-5 matches WITH BruteForce cleanup:\")\nfor i, (label, sim) in enumerate(zip(labels_bf, sims_bf), 1):\n    marker = \" \u2190 Target!\" if label == target_key else \"\"\n    print(f\"  {i}. {label:12s}: {sim:.3f}{marker}\")\n\n# With Resonator cleanup\nres_cleanup = ResonatorCleanup()\nlabels_res, sims_res = res_cleanup.factorize(corrupted, codebook, model,\n                                              n_factors=5, max_iterations=10)\n\nprint(\"\\nTop-5 matches WITH Resonator cleanup:\")\nfor i, (label, sim) in enumerate(zip(labels_res, sims_res), 1):\n    marker = \" \u2190 Target!\" if label == target_key else \"\"\n    print(f\"  {i}. {label:12s}: {sim:.3f}{marker}\")\n\nprint(\"\\nCleanup improves robustness significantly!\")\n\n# ============================================================================\n# Demo 7: Redundancy Strategies\n# ============================================================================\nprint(\"\\n\" + \"=\" * 70)\nprint(\"Demo 7: Redundancy for Fault Tolerance\")\nprint(\"=\" * 70)\n\nmodel = VSA.create('MAP', dim=10000, seed=42)\n\n# Store data with redundancy\ndata = model.random(seed=1)\n\n# Repeat encoding (redundancy)\nredundancy_levels = [1, 3, 5, 10]\ncorruption_rate = 0.2\n\nprint(f\"\\nCorruption rate: {corruption_rate * 100:.0f}%\")\nprint()\n\nprint(f\"{'Redundancy':<12s} {'Similarity':<15s} {'Recovery Quality':<18s}\")\nprint(\"-\" * 52)\n\nfor redundancy in redundancy_levels:\n    # Create redundant copies\n    copies = [data] * redundancy\n\n    # Bundle them\n    redundant_storage = model.bundle(copies)\n\n    # Corrupt the storage\n    noise = model.random(seed=999)\n    corrupted = model.bundle([redundant_storage] * int(100 * (1 - corruption_rate)) +\n                              [noise] * int(100 * corruption_rate))\n\n    # Check similarity to original\n    sim = float(model.similarity(corrupted, data))\n    quality = \"Excellent\" if sim > 0.8 else \"Good\" if sim > 0.6 else \"Poor\"\n\n    print(f\"{redundancy:10d}x   {sim:13.3f}   {quality:<18s}\")\n\nprint(\"\\nRedundancy helps but has diminishing returns:\")\nprint(\"  - 3x redundancy provides good protection\")\nprint(\"  - Beyond 5x, limited additional benefit\")\nprint(\"  - Trade-off: robustness vs storage efficiency\")\n\n# ============================================================================\n# Summary\n# ============================================================================\nprint(\"\\n\" + \"=\" * 70)\nprint(\"Summary: Robustness Best Practices\")\nprint(\"=\" * 70)\nprint()\n\nprint(\"\u2713 Noise Tolerance Strategies:\")\nprint(\"  1. Use adequate dimension (10k+ for noisy environments)\")\nprint(\"  2. Apply cleanup on retrieval (BruteForce or Resonator)\")\nprint(\"  3. Bundle multiple noisy samples (sensor fusion)\")\nprint(\"  4. Add redundancy for critical data\")\nprint()\n\nprint(\"\u2713 Error Recovery:\")\nprint(\"  - Detect: Monitor similarity scores\")\nprint(\"  - Recover: Use cleanup strategies\")\nprint(\"  - Prevent: Higher dimension, redundancy\")\nprint(\"  - Degrade gracefully: Always get approximate answer\")\nprint()\n\nprint(\"\u2713 Design Guidelines:\")\nprint(\"  - Expected noise < 10%: Standard dimension (10k)\")\nprint(\"  - Expected noise 10-20%: Higher dimension (20k) + cleanup\")\nprint(\"  - Expected noise > 20%: Redundancy + aggressive cleanup\")\nprint(\"  - Critical applications: Add error-correcting codes\")\nprint()\n\nprint(\"\u2713 HDC Advantages for Robust Systems:\")\nprint(\"  - Graceful degradation (no catastrophic failures)\")\nprint(\"  - Natural fault tolerance (distributed representation)\")\nprint(\"  - Simple error recovery (cleanup strategies)\")\nprint(\"  - Predictable behavior under stress\")\nprint()\n\nprint(\"Next steps:\")\nprint(\"  \u2192 27_cleanup_strategies.py - Detailed cleanup methods\")\nprint(\"  \u2192 32_distributed_representations.py - Capacity under noise\")\nprint(\"  \u2192 31_performance_benchmarks.py - Dimension cost analysis\")\nprint()\nprint(\"=\" * 70)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}