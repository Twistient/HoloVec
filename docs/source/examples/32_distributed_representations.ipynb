{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# Distributed Representations and Capacity Analysis\n\nTopics: Bundling capacity, dimension effects, information limits, cleanup\nTime: 15 minutes\nPrerequisites: 01_basic_operations.py, 02_models_comparison.py\nRelated: 31_performance_benchmarks.py, 27_cleanup_strategies.py\n\nThis example explores the fundamental capacity limits of hyperdimensional\ncomputing - how many items can be bundled together while maintaining\nretrievability, and how dimension affects this capacity.\n\nKey concepts:\n- Bundling capacity: Number of items in superposition\n- Dimension scaling: More dimensions \u2192 higher capacity\n- Similarity decay: How bundling reduces similarity\n- Cleanup requirements: When do we need cleanup?\n- Practical limits: Real-world capacity guidelines\n\nUnderstanding capacity is crucial for designing reliable HDC systems.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import numpy as np\nfrom holovec import VSA\nfrom holovec.utils.cleanup import BruteForceCleanup\n\nprint(\"=\" * 70)\nprint(\"Distributed Representations and Capacity Analysis\")\nprint(\"=\" * 70)\nprint()\n\n# ============================================================================\n# Demo 1: Basic Bundling Capacity\n# ============================================================================\nprint(\"=\" * 70)\nprint(\"Demo 1: Bundling Capacity - Similarity Decay\")\nprint(\"=\" * 70)\n\nmodel = VSA.create('MAP', dim=10000, seed=42)\n\nprint(f\"\\nModel: {model.model_name}\")\nprint(f\"Dimension: {model.dimension}\")\nprint()\n\n# Test bundling different numbers of vectors\nbundle_sizes = [1, 2, 5, 10, 20, 50, 100, 200]\n\nprint(f\"{'Bundle Size':<15s} {'Avg Similarity':<15s} {'Min Similarity':<15s} {'Retrievable?':<12s}\")\nprint(\"-\" * 65)\n\nfor n in bundle_sizes:\n    # Create n vectors\n    vectors = [model.random(seed=i) for i in range(n)]\n\n    # Bundle them\n    bundled = model.bundle(vectors)\n\n    # Test similarity to each original\n    similarities = [float(model.similarity(bundled, v)) for v in vectors]\n    avg_sim = np.mean(similarities)\n    min_sim = np.min(similarities)\n\n    # Heuristic: retrievable if min similarity > 0.3\n    retrievable = \"Yes\" if min_sim > 0.3 else \"Marginal\" if min_sim > 0.15 else \"No\"\n\n    print(f\"{n:<15d} {avg_sim:13.3f}   {min_sim:13.3f}   {retrievable:<12s}\")\n\nprint(\"\\nKey insight:\")\nprint(\"  - Similarity decreases as \u221a(1/n) approximately\")\nprint(\"  - With dim=10000, can reliably bundle ~50-100 items\")\nprint(\"  - Beyond that, cleanup strategies needed\")\n\n# ============================================================================\n# Demo 2: Dimension vs Capacity\n# ============================================================================\nprint(\"\\n\" + \"=\" * 70)\nprint(\"Demo 2: How Dimension Affects Capacity\")\nprint(\"=\" * 70)\n\ndimensions = [1000, 5000, 10000, 20000]\ntest_bundle_sizes = [10, 20, 50, 100]\n\nprint(f\"\\n{'Dimension':<12s} \", end=\"\")\nfor n in test_bundle_sizes:\n    print(f\"N={n:<8d} \", end=\"\")\nprint()\nprint(\"-\" * 60)\n\nfor dim in dimensions:\n    model = VSA.create('MAP', dim=dim, seed=42)\n    print(f\"{dim:<12d} \", end=\"\")\n\n    for n in test_bundle_sizes:\n        vectors = [model.random(seed=i) for i in range(n)]\n        bundled = model.bundle(vectors)\n\n        # Average similarity\n        sims = [float(model.similarity(bundled, v)) for v in vectors]\n        avg_sim = np.mean(sims)\n\n        print(f\"{avg_sim:9.3f} \", end=\"\")\n\n    print()\n\nprint(\"\\nObservations:\")\nprint(\"  - Higher dimension \u2192 higher similarity for same bundle size\")\nprint(\"  - Roughly: capacity \u221d dimension\")\nprint(\"  - dim=10000 supports ~100 items comfortably\")\nprint(\"  - dim=20000 supports ~200 items\")\n\n# ============================================================================\n# Demo 3: Information Theory Perspective\n# ============================================================================\nprint(\"\\n\" + \"=\" * 70)\nprint(\"Demo 3: Information Capacity\")\nprint(\"=\" * 70)\n\nmodel = VSA.create('MAP', dim=10000, seed=42)\n\nprint(f\"\\nDimension: {model.dimension}\")\nprint(f\"\\nInformation capacity analysis:\")\nprint()\n\n# Each bit in MAP can be +1 or -1 (1 bit of info)\ntotal_bits = model.dimension\n\nprint(f\"Total bits available: {total_bits:,}\")\nprint()\n\n# Bundling n vectors averages them\n# Information per vector decreases with bundle size\nprint(f\"{'Bundle Size':<15s} {'Bits/Vector':<15s} {'Total Bits Used':<18s}\")\nprint(\"-\" * 55)\n\nfor n in [1, 10, 50, 100, 200]:\n    bits_per_vector = total_bits / n\n    total_used = total_bits  # Bundle still uses full dimension\n\n    print(f\"{n:<15d} {bits_per_vector:13.1f}   {total_used:16,d}\")\n\nprint(\"\\nKey insight:\")\nprint(\"  - Fixed total capacity (dimension)\")\nprint(\"  - More vectors = less information per vector\")\nprint(\"  - Trade-off: quantity vs fidelity\")\n\n# ============================================================================\n# Demo 4: Retrieval Accuracy vs Bundle Size\n# ============================================================================\nprint(\"\\n\" + \"=\" * 70)\nprint(\"Demo 4: Retrieval Accuracy Analysis\")\nprint(\"=\" * 70)\n\nmodel = VSA.create('MAP', dim=10000, seed=42)\n\n# Create a codebook\ncodebook_size = 100\ncodebook = {f\"item_{i}\": model.random(seed=1000+i)\n            for i in range(codebook_size)}\n\ncleanup = BruteForceCleanup()\n\nprint(f\"\\nCodebook size: {codebook_size}\")\nprint(f\"Testing retrieval accuracy for different bundle sizes:\")\nprint()\n\nbundle_sizes_test = [5, 10, 20, 50, 100]\n\nprint(f\"{'Bundle Size':<15s} {'Accuracy':<12s} {'Avg Rank':<12s}\")\nprint(\"-\" * 45)\n\nfor n in bundle_sizes_test:\n    # Select random items to bundle\n    np.random.seed(42)\n    selected_keys = np.random.choice(list(codebook.keys()), size=n, replace=False)\n    selected = [codebook[key] for key in selected_keys]\n\n    # Bundle\n    bundled = model.bundle(selected)\n\n    # Try to retrieve each item\n    correct = 0\n    ranks = []\n\n    for target_key in selected_keys:\n        target = codebook[target_key]\n\n        # Find most similar in codebook\n        best_key = None\n        best_sim = float('-inf')\n\n        for key, vec in codebook.items():\n            sim = float(model.similarity(bundled, vec))\n            if sim > best_sim:\n                best_sim = sim\n                best_key = key\n\n        if best_key == target_key:\n            correct += 1\n\n        # Calculate rank\n        sims = [(key, float(model.similarity(bundled, vec)))\n                for key, vec in codebook.items()]\n        sims.sort(key=lambda x: x[1], reverse=True)\n        rank = next(i for i, (key, _) in enumerate(sims, 1) if key == target_key)\n        ranks.append(rank)\n\n    accuracy = correct / n\n    avg_rank = np.mean(ranks)\n\n    print(f\"{n:<15d} {accuracy:10.2%}   {avg_rank:10.1f}\")\n\nprint(\"\\nInsight:\")\nprint(\"  - Accuracy drops as bundle size increases\")\nprint(\"  - Even when top-1 fails, true item often in top-10\")\nprint(\"  - Cleanup strategies help improve accuracy\")\n\n# ============================================================================\n# Demo 5: Binding vs Bundling Capacity\n# ============================================================================\nprint(\"\\n\" + \"=\" * 70)\nprint(\"Demo 5: Binding Chains - Different Capacity Behavior\")\nprint(\"=\" * 70)\n\nmodel = VSA.create('FHRR', dim=10000, seed=42)\n\nprint(f\"\\nModel: {model.model_name} (exact inverses)\")\nprint()\n\n# Test binding chains of different lengths\nchain_lengths = [1, 2, 3, 5, 10]\n\nprint(f\"{'Chain Length':<15s} {'Similarity':<15s} {'Recoverable?':<12s}\")\nprint(\"-\" * 50)\n\nfor length in chain_lengths:\n    # Create binding chain: A * B * C * D * ...\n    vectors = [model.random(seed=100+i) for i in range(length)]\n\n    # Bind them all\n    result = vectors[0]\n    for v in vectors[1:]:\n        result = model.bind(result, v)\n\n    # Try to recover first vector by unbinding all others\n    recovered = result\n    for v in reversed(vectors[1:]):\n        recovered = model.unbind(recovered, v)\n\n    # Similarity to original\n    sim = float(model.similarity(recovered, vectors[0]))\n    recoverable = \"Yes\" if sim > 0.99 else \"Degraded\"\n\n    print(f\"{length:<15d} {sim:13.3f}   {recoverable:<12s}\")\n\nprint(\"\\nKey difference:\")\nprint(\"  - Binding (with exact inverses): Perfect recovery regardless of length\")\nprint(\"  - Bundling: Similarity degrades with number of items\")\nprint(\"  - Use binding for structured data, bundling for sets\")\n\n# ============================================================================\n# Demo 6: Practical Capacity Guidelines\n# ============================================================================\nprint(\"\\n\" + \"=\" * 70)\nprint(\"Demo 6: Practical Capacity Guidelines\")\nprint(\"=\" * 70)\n\nprint(\"\\nRule of thumb for bundling capacity:\")\nprint()\n\nguideline_table = \"\"\"\nDimension    No Cleanup    With Cleanup    Use Case\n\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n1,000        ~10 items     ~20 items       Toy problems\n5,000        ~50 items     ~100 items      Small applications\n10,000       ~100 items    ~200 items      Standard applications\n20,000       ~200 items    ~500 items      Large-scale systems\n\"\"\"\n\nprint(guideline_table)\n\nprint(\"\\nFactors affecting capacity:\")\nprint(\"  1. Dimension: Higher = more capacity\")\nprint(\"  2. Required similarity: Lower threshold = more capacity\")\nprint(\"  3. Cleanup: Can double effective capacity\")\nprint(\"  4. Model: MAP vs FHRR may differ slightly\")\nprint(\"  5. Noise level: Clean data = higher capacity\")\n\n# ============================================================================\n# Demo 7: Overloading Recovery Strategies\n# ============================================================================\nprint(\"\\n\" + \"=\" * 70)\nprint(\"Demo 7: Strategies for High-Capacity Scenarios\")\nprint(\"=\" * 70)\n\nprint(\"\\nWhen you need to store more items than dimension allows:\\n\")\n\nprint(\"Strategy 1: Multiple bundles (sharding)\")\nprint(\"  - Split items into groups\")\nprint(\"  - Bundle each group separately\")\nprint(\"  - Store multiple bundles\")\nprint(\"  Example: 1000 items \u2192 10 bundles of 100 each\")\nprint()\n\nprint(\"Strategy 2: Hierarchical encoding\")\nprint(\"  - Create categories\")\nprint(\"  - Bundle within categories\")\nprint(\"  - Bind category to bundle\")\nprint(\"  Example: colors\u2192[red, blue] spatial\u2192[top, bottom]\")\nprint()\n\nprint(\"Strategy 3: Cleanup on retrieval\")\nprint(\"  - Allow lower bundling similarity\")\nprint(\"  - Use resonator cleanup to sharpen\")\nprint(\"  - See 27_cleanup_strategies.py\")\nprint()\n\nprint(\"Strategy 4: Increase dimension\")\nprint(\"  - dim=20000 or dim=50000\")\nprint(\"  - Trade memory for capacity\")\nprint(\"  - See 31_performance_benchmarks.py for cost\")\n\n# ============================================================================\n# Summary\n# ============================================================================\nprint(\"\\n\" + \"=\" * 70)\nprint(\"Summary: Capacity Recommendations\")\nprint(\"=\" * 70)\nprint()\n\nprint(\"\u2713 Bundling Capacity Rules:\")\nprint(\"  - Conservative: N \u2264 dimension / 100\")\nprint(\"  - Standard: N \u2264 dimension / 50 (with cleanup)\")\nprint(\"  - Aggressive: N \u2264 dimension / 20 (requires strong cleanup)\")\nprint()\n\nprint(\"\u2713 When Approaching Limits:\")\nprint(\"  - Monitor similarity metrics\")\nprint(\"  - Test retrieval accuracy\")\nprint(\"  - Use cleanup strategies\")\nprint(\"  - Consider hierarchical encoding\")\nprint()\n\nprint(\"\u2713 Dimension Selection:\")\nprint(\"  - Estimate max bundle size needed\")\nprint(\"  - Choose dim \u2265 50 \u00d7 max_bundle_size\")\nprint(\"  - Leave safety margin (2x)\")\nprint(\"  Example: Need 100 items \u2192 dim \u2265 10,000\")\nprint()\n\nprint(\"\u2713 Binding Capacity:\")\nprint(\"  - Much higher than bundling (with exact inverses)\")\nprint(\"  - Limited by numerical precision, not information\")\nprint(\"  - Chains of 10-20 bindings work well\")\nprint()\n\nprint(\"Next steps:\")\nprint(\"  \u2192 27_cleanup_strategies.py - Improve retrieval accuracy\")\nprint(\"  \u2192 33_error_handling_robustness.py - Handle degraded signals\")\nprint(\"  \u2192 31_performance_benchmarks.py - Dimension cost analysis\")\nprint()\nprint(\"=\" * 70)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}