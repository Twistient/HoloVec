{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# Document Classification with N-grams\n\nTopics: Text classification, n-gram encoding, supervised learning, NLP\nTime: 15 minutes\nPrerequisites: 14_encoders_ngram.py, 26_retrieval_basics.py\nRelated: 23_app_symbolic_reasoning.py, 25_app_integration_patterns.py\n\nThis example demonstrates practical document classification using n-gram\nencoding and hyperdimensional computing. Learn how to build a text classifier\nthat can categorize documents based on their content.\n\nKey concepts:\n- Document encoding: N-gram patterns for text representation\n- Training: Build class prototypes from labeled examples\n- Classification: Nearest-prototype matching\n- Evaluation: Accuracy metrics and confusion analysis\n\nText classification with HDC is fast, interpretable, and works well with\nlimited training data - ideal for practical NLP applications.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from holovec import VSA\nfrom holovec.encoders import NGramEncoder\nfrom holovec.retrieval import ItemStore\n\nprint(\"=\" * 70)\nprint(\"Document Classification with N-grams\")\nprint(\"=\" * 70)\nprint()\n\n# Create model and encoder\nmodel = VSA.create('MAP', dim=10000, seed=42)\nencoder = NGramEncoder(model, n=3, stride=1, mode='bundling', seed=42)\n\nprint(f\"Model: {model.model_name}, dimension={model.dimension}\")\nprint(f\"Encoder: NGramEncoder(n=3, mode='bundling')\")\nprint()\n\n# ============================================================================\n# Dataset: News Article Classification\n# ============================================================================\nprint(\"=\" * 70)\nprint(\"Dataset: News Articles (4 categories)\")\nprint(\"=\" * 70)\n\n# Training data: short news snippets\ntraining_data = [\n    # Sports\n    (\"team wins championship game tonight\", \"sports\"),\n    (\"player scores winning goal match\", \"sports\"),\n    (\"coach announces new training strategy\", \"sports\"),\n    (\"league announces playoff schedule games\", \"sports\"),\n\n    # Technology\n    (\"new smartphone launched today features\", \"technology\"),\n    (\"software update fixes security bug\", \"technology\"),\n    (\"tech company releases ai chatbot\", \"technology\"),\n    (\"startup raises million funding round\", \"technology\"),\n\n    # Business\n    (\"stock market rises today investors\", \"business\"),\n    (\"company reports quarterly earnings profit\", \"business\"),\n    (\"merger deal announced billion dollars\", \"business\"),\n    (\"economic growth forecast next quarter\", \"business\"),\n\n    # Health\n    (\"new study shows health benefits\", \"health\"),\n    (\"doctor recommends exercise diet plan\", \"health\"),\n    (\"hospital opens new emergency wing\", \"health\"),\n    (\"vaccine approved clinical trials results\", \"health\"),\n]\n\nprint(f\"\\nTraining examples: {len(training_data)} articles\")\nprint(f\"Categories: sports, technology, business, health\")\nprint(f\"Examples per category: {len(training_data) // 4}\")\nprint()\n\n# ============================================================================\n# Training: Build Class Prototypes\n# ============================================================================\nprint(\"=\" * 70)\nprint(\"Training: Building Class Prototypes\")\nprint(\"=\" * 70)\n\n# Group examples by category\ncategories = {}\nfor text, label in training_data:\n    if label not in categories:\n        categories[label] = []\n    categories[label].append(text)\n\nprint(\"\\nEncoding training examples...\")\n\n# Build prototype for each category\nclass_prototypes = {}\nfor label, texts in categories.items():\n    # Encode all documents in this category\n    encoded_docs = [encoder.encode(text) for text in texts]\n\n    # Bundle to create class prototype\n    prototype = model.bundle(encoded_docs)\n    class_prototypes[label] = prototype\n\n    print(f\"  {label:12s}: {len(texts)} examples \u2192 prototype\")\n\nprint(f\"\\nClass prototypes created: {len(class_prototypes)}\")\n\n# ============================================================================\n# Classification: Predict Category for New Documents\n# ============================================================================\nprint(\"\\n\" + \"=\" * 70)\nprint(\"Classification: Testing on New Documents\")\nprint(\"=\" * 70)\n\n# Test documents\ntest_documents = [\n    \"basketball team defeats rivals final\",  # sports\n    \"new laptop computer faster processor\",  # technology\n    \"company profits increase stock price\",  # business\n    \"patients recover hospital treatment\",   # health\n]\n\nprint(\"\\nClassifying test documents:\")\nprint()\n\ncorrect = 0\ntotal = len(test_documents)\n\nexpected_labels = [\"sports\", \"technology\", \"business\", \"health\"]\n\nfor i, doc in enumerate(test_documents):\n    # Encode test document\n    doc_hv = encoder.encode(doc)\n\n    # Find most similar class prototype\n    best_label = None\n    best_sim = float('-inf')\n\n    for label, prototype in class_prototypes.items():\n        sim = float(model.similarity(doc_hv, prototype))\n        if sim > best_sim:\n            best_sim = sim\n            best_label = label\n\n    expected = expected_labels[i]\n    is_correct = best_label == expected\n    correct += (1 if is_correct else 0)\n\n    marker = \"\u2713\" if is_correct else \"\u2717\"\n    print(f\"{i+1}. \\\"{doc}\\\"\")\n    print(f\"   Predicted: {best_label:12s} (similarity={best_sim:.3f}) {marker}\")\n    print(f\"   Expected:  {expected:12s}\")\n    print()\n\naccuracy = correct / total\nprint(f\"Accuracy: {correct}/{total} = {accuracy:.1%}\")\n\n# ============================================================================\n# Analysis: Understanding Classification Decisions\n# ============================================================================\nprint(\"\\n\" + \"=\" * 70)\nprint(\"Analysis: Classification Confidence\")\nprint(\"=\" * 70)\n\nprint(\"\\nDetailed similarity scores for first test doc:\")\nprint(f\"Document: '{test_documents[0]}'\")\nprint()\n\ndoc_hv = encoder.encode(test_documents[0])\n\nprint(f\"{'Category':12s} | {'Similarity':>12s} | {'Confidence':>12s}\")\nprint(\"-\" * 45)\n\nfor label in sorted(class_prototypes.keys()):\n    sim = float(model.similarity(doc_hv, class_prototypes[label]))\n    conf = \"High\" if sim > 0.6 else \"Medium\" if sim > 0.4 else \"Low\"\n    print(f\"{label:12s} | {sim:12.3f} | {conf:>12s}\")\n\nprint(\"\\nKey observation:\")\nprint(\"  - Clear winner indicates confident classification\")\nprint(\"  - Similar scores suggest ambiguous document\")\nprint(\"  - Low scores across all classes suggest out-of-domain\")\n\n# ============================================================================\n# Practical Considerations\n# ============================================================================\nprint(\"\\n\" + \"=\" * 70)\nprint(\"Practical Considerations\")\nprint(\"=\" * 70)\n\nprint(\"\\n\u2713 Advantages of HDC Text Classification:\")\nprint(\"  - Fast training: just bundle examples per class\")\nprint(\"  - One-shot learning: works with few examples\")\nprint(\"  - Interpretable: similarity scores show confidence\")\nprint(\"  - Noise tolerant: robust to typos and variations\")\nprint(\"  - No gradient descent: no hyperparameter tuning\")\nprint()\n\nprint(\"\u2717 Limitations:\")\nprint(\"  - Approximate: not as accurate as deep learning (large data)\")\nprint(\"  - Capacity: performance degrades with many classes\")\nprint(\"  - Context: n-grams miss long-range dependencies\")\nprint(\"  - Tuning: n-gram size affects performance\")\nprint()\n\nprint(\"When to use HDC text classification:\")\nprint(\"  - Limited training data (< 100 examples per class)\")\nprint(\"  - Fast deployment needed (no training time)\")\nprint(\"  - Interpretability important (need similarity scores)\")\nprint(\"  - Edge devices (low compute, memory constraints)\")\nprint(\"  - Prototyping (quick baseline before deep learning)\")\nprint()\n\n# ============================================================================\n# Extension: Using ItemStore for Efficient Classification\n# ============================================================================\nprint(\"=\" * 70)\nprint(\"Extension: ItemStore for Multi-Class Classification\")\nprint(\"=\" * 70)\n\n# Build ItemStore with class prototypes\nclassifier = ItemStore(model)\nfor label, prototype in class_prototypes.items():\n    classifier.add(label, prototype)\n\nprint(f\"\\nClassifier built with {len(class_prototypes)} classes\")\n\n# Classify with ItemStore\nprint(\"\\nClassifying with ItemStore:\")\n\ntest_doc = \"scientist discovers new medical treatment\"\ntest_hv = encoder.encode(test_doc)\n\n# Query returns list of (label, similarity) tuples\nresults = classifier.query(test_hv, k=4)\n\nprint(f\"\\nDocument: '{test_doc}'\")\nprint(\"\\nTop predictions:\")\nfor i, (label, sim) in enumerate(results, 1):\n    print(f\"  {i}. {label:12s}: {sim:.3f}\")\n\nprint(\"\\nKey observation:\")\nprint(\"  - ItemStore enables efficient k-nearest class retrieval\")\nprint(\"  - Can examine top-k predictions for confidence\")\nprint(\"  - Scales well to many classes (1000+)\")\n\n# ============================================================================\n# Summary\n# ============================================================================\nprint(\"\\n\" + \"=\" * 70)\nprint(\"Summary: Text Classification with HDC\")\nprint(\"=\" * 70)\nprint()\nprint(\"Complete workflow:\")\nprint(\"  1. Setup: Create model + NGramEncoder\")\nprint(\"  2. Training: Encode examples + bundle per class\")\nprint(\"  3. Classification: Encode test doc + find nearest prototype\")\nprint(\"  4. Evaluation: Check similarity scores for confidence\")\nprint()\nprint(\"Performance tips:\")\nprint(\"  - N-gram size: 2-3 for words, 3-5 for characters\")\nprint(\"  - More training examples \u2192 better prototypes\")\nprint(\"  - Bundle diverse examples to capture class variation\")\nprint(\"  - Use ItemStore for efficient multi-class problems\")\nprint()\nprint(\"Next steps:\")\nprint(\"  \u2192 Try with your own text dataset\")\nprint(\"  \u2192 Experiment with n-gram sizes (n=2, 3, 4)\")\nprint(\"  \u2192 Combine with 25_app_integration_patterns.py for multimodal\")\nprint(\"  \u2192 Scale to larger datasets with ItemStore\")\nprint()\nprint(\"=\" * 70)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}